<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>SunShining</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="A blog of a machine learning learner @ShanghaiTech University">
<meta property="og:type" content="website">
<meta property="og:title" content="SunShining">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="SunShining">
<meta property="og:description" content="A blog of a machine learning learner @ShanghaiTech University">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SunShining">
<meta name="twitter:description" content="A blog of a machine learning learner @ShanghaiTech University">
  
    <link rel="alternative" href="/atom.xml" title="SunShining" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  <script src="http://libs.baidu.com/jquery/1.9.0/jquery.js"></script>
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img lazy-src="http://ww1.sinaimg.cn/mw690/6dfbc26fjw1erww25vfwbj208709ddge.jpg" class="js-avatar">
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Yong Jiang</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						<li>友情链接</li>
						
						
						<li>关于我</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="/#" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="http://weibo.com/u/1845215855" title="weibo">weibo</a>
					        
								<a class="rss" target="_blank" href="/#" title="rss">rss</a>
					        
								<a class="zhihu" target="_blank" href="/#" title="zhihu">zhihu</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Deep-learning/" style="font-size: 15px;">Deep learning</a><a href="/tags/Generalized-Linear-Model/" style="font-size: 15px;">Generalized Linear Model</a><a href="/tags/Linear-Model/" style="font-size: 15px;">Linear Model</a><a href="/tags/Logistic-regression/" style="font-size: 10px;">Logistic regression</a><a href="/tags/Machine-learning/" style="font-size: 20px;">Machine learning</a><a href="/tags/Naive-bayes/" style="font-size: 10px;">Naive bayes</a><a href="/tags/Natural-language-processing/" style="font-size: 15px;">Natural language processing</a><a href="/tags/Parsing/" style="font-size: 15px;">Parsing</a><a href="/tags/Word-Embedding/" style="font-size: 15px;">Word Embedding</a><a href="/tags/blog/" style="font-size: 10px;">blog</a><a href="/tags/learning/" style="font-size: 10px;">learning</a>
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="http://frankchu0229.github.io/">Frank Chu&#39;s blog</a>
			        
			          <a target="_blank" class="main-nav-link switch-friends-link" href="http://zhaoyanpeng.github.io/">小小小师弟</a>
			        
			        </div>
				</section>
				

				
				
				<section class="switch-part switch-part4">
				
					<div id="js-aboutme">Now I am a graduate student in ShanghaiTech University. I am doing some research projects about Machine learning.</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Yong Jiang</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="http://ww1.sinaimg.cn/mw690/6dfbc26fjw1erww25vfwbj208709ddge.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">Yong Jiang</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="/#" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="http://weibo.com/u/1845215855" title="weibo">weibo</a>
			        
						<a class="rss" target="_blank" href="/#" title="rss">rss</a>
			        
						<a class="zhihu" target="_blank" href="/#" title="zhihu">zhihu</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-Word-Embedding-An-Introduction-and-Its-Application-in-Sentence-Parsing-Part-2" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/05/16/Word-Embedding-An-Introduction-and-Its-Application-in-Sentence-Parsing-Part-2/" class="article-date">
  	<time datetime="2015-05-16T08:16:41.000Z" itemprop="datePublished">2015-05-16</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/16/Word-Embedding-An-Introduction-and-Its-Application-in-Sentence-Parsing-Part-2/">Word Embedding:An Introduction and Its Application in Sentence Parsing - Part 2</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>续上，这篇博客我们来讨论一下基于迭代策略的词向量的学习。主要想介绍一下四个相关的神经网络来学词向量。</p>
<h1 id="1-_语言模型（Language_Modeling）">1. 语言模型（Language Modeling）</h1><p>所有用在NLP中的神经网络模型都是基于语言模型来构建的。我们知道，一篇文章是由一系列的句子组成，每个句子都是由一系列的词组成。我们现在有单独的词（Atom），我们希望构建一系列的句子，从而构建整篇文章。在自然语言处理中有一个很出名的模型叫做N-gram 模型，它是将马尔科夫假设（Makov Assumptions）用在句子的生成过程中。例如这么一个句子：</p>
<blockquote>
<p>I am a graduate student in ShanghaiTech University.</p>
</blockquote>
<h2 id="1-1-单纯的词袋模型（Bag_of_Words）">1.1.单纯的词袋模型（Bag of Words）</h2><p>有图有真相！前一阵在微博上面看到下面的这个图很有意思。<br><img src="http://ww2.sinaimg.cn/mw690/6dfbc26fjw1es669t3064j20ce0higpi.jpg" alt="Bag of Words Model"><br>BoW是指，所有的词之间相互没有关系，即在构建整个句子中不考虑词与词的顺序。因此，上面的句子跟下面的句子本质上表达的意义相同：</p>
<blockquote>
<p>University student am graduate I ShanghaiTech a in.</p>
</blockquote>
<p>这看起来很荒唐，但实际上像上一篇博客所说的LDA模型就是这么做的，在做主题建模时work的非常好。当然，这是看应用的。在复杂的问题下如何做出<code>正确的Assumption</code>正是Machine Learning想要研究的问题。对于上面这个句子，我们计算产生它的概率，可以通过这些词的联合概率来计算：<br>$$P\left(w^{(1)},w^{(2)},…,w^{(n)}\right)=\prod_{i=1}^{n}P\left(w^{(i)}\right)$$</p>
<p>但是，在产生句子这个问题时，Bags of Words模型肯定不够好，因为人在理解一句话中的某个词都会依靠其上下文的关系。因此我们可以把词与其前面n-1个词的关系加进来，这就是n-gram模型。</p>
<h2 id="1-2-Binary_Gram_&amp;_Triple_Gram">1.2.Binary Gram &amp; Triple Gram</h2><p>很简单，在计算一个句子产生的概率时把每个词与其前面n-1个词的关系考虑进来，可以得到：<br>Binary Gram:$P\left(w^{(1)},w^{(2)},…,w^{(n)}\right)=\prod_{i=2}^{n}P\left(w^{(i)}|w^{(i-1)}\right)P\left(w^{(1)}\right)$<br>Triple Gram:$P\left(w^{(1)},w^{(2)},…,w^{(n)}\right)=\prod_{i=3}^{n}P\left(w^{(i)}|w^{(i-1)},w^{(i-2)}\right)P\left(w^{(2)}|w^{(1)}\right)P\left(w^{(1)}\right)$<br>同样可以考虑前n-1个词的关系，但是，随着n的增大，模型复杂度也会随之增大。因为，实际应用中如何选择一个好的n也很有讲究，这在Machine Learning中被称为Bias-Variance Trade-off。实际应用中，我们组的师兄一般用Binary Gram或者Triple Gram就可以表达足够的信息了。</p>
<h1 id="2-A_Simple_Neural_Network_Model_for_Word_Embedding">2.A Simple Neural Network Model for Word Embedding</h1><p>我们在既然讨论了语言模型，就想把语言模型的idea用来构建神经网络，以此对词进行表示。在n-gram模型中，我们用前n-1个词来对第n个词进行表示。那么在神经网络中我们也可以这么做，我们的输入是n-1个词的One-hot Vectors，预测输出是第n个词。如何在一个句子中有多少个这样的神经网络呢，我们来举个例子，对于下面这个句子：</p>
<blockquote>
<p>Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing where words from the vocabulary…</p>
</blockquote>
<p>我们可以构建多个窗口（window），每个窗口的大小是n。因此随着窗口的移动，一个句子可以得到$len \left(sentence\right)-n$个窗口，如下：</p>
<blockquote>
<p><code>Word embedding is the</code> collective name for a set of language modeling and feature learning techniques in natural language processing where words from the vocabulary…<br>Word <code>embedding is the collective</code> name for a set of language modeling and feature learning techniques in natural language processing where words from the vocabulary…<br>Word embedding <code>is the collective name</code> for a set of language modeling and feature learning techniques in natural language processing where words from the vocabulary…<br>…</p>
</blockquote>
<p>对于每一个这样的窗口，我们可以构建一个神经网络来对其中的语言模型进行建模。这就是Bengio等人2003年的工作：<br><img src="http://ww1.sinaimg.cn/mw690/6dfbc26fjw1es56311p47j20pt0ltq7a.jpg" alt="Simple Neural Network Model"><br>输入层有n-1个词，输出层包括dictionary中所有词的概率。输入层到隐藏层是通过一个Tanh函数来映射。算出Tanh后的假设我们希望在对每个词用一个$d$维度的向量来表示。因此输入的One-hot vector 通过一个look up table转换成d维的词向量表示（神经网络中的第一层到第二层）。从第二层到第三层是指将前n-1个词向量左乘一个H矩阵，映射到隐藏层。隐藏层到输出层是一个线性映射。因此我们可以得到每一个小窗口的目标函数：<br>$$f\left(w_c,w_{c-1},…,w_{c-n+1}\right)=P\left(w_c|w_{c-1},…,w_{c-n+1}\right)=\frac{e^{y_{w_c}}}{\sum_i e^{y_i}}$$<br>对于每个句子，我们有$len \left(sentence\right)-n$个窗口，那对于所有的句子来说，假设一共有$C$个窗口，则我们可以得到目标函数：<br>$$J(\theta)=\frac{1}{C}\sum_{c=n}^{C}\log f(w_c,w_{c-1},…,w_{c-n+1};\theta)+R(\theta)$$<br>每一个$f$都可以用上面的式子来算，$R(\theta)$是正则项，在神经网络，尤其是深度学习中很常见，一般是参数的$l_2$范数，其目的为了防止overfitting。对于每一个$y_{w_c}$我们应该如何求呢? 就像刚才所说，求这个output的过程就是一个forward process，我们想求所有的$y_{w_c}$，即该神经网络的最后一层：<br>$$y=b+Wx+Utanh(d+Hx)$$<br>令隐藏层的数目为$h$，$m$是每个词的词向量表示。解释一下上个式子中符号的意思，$b\in R^{|V|}$是词向量到output的偏置项（Bias term）。$d\in R^{h}$<br>是词向量这一层到output层的偏置项。$U\in R^{|V|\times h}$是隐藏层到output的weight matrix。词向量层到output层的weight是矩阵$W\in R^{|V|\times \left(n-1\right)m}$。隐藏层算出来的权重$H\in R^{h \times \left(n-1\right)m}$。词向量层的矩阵值是$C\in R^{|V|\times m}$，这个矩阵其实就是词向量的表示。<br>这里的式子比我们刚才所说的forward process多了一项，即$Wx$。在Bengio的paper中他解释到添加这一层可以使得训练速度更快，但是使得神经网络学习到更好的参数，因此$W$可以设置为0。<br>得到整个的目标函数，我们希望求解所有的参数:$W,U,x,d,b$。用老方法：Stochastic Gradient Desent。我们在接下来的博客里会举个例子来看一下一般神经网络模型该如何学习参数。</p>
<h1 id="3-Continuous_Bag_of_Words_Model">3.Continuous Bag of Words Model</h1><p>Mikolov等人在谷歌提出了两个计算词向量的模型，分别是CBOW和skip-gram模型，我们在这一部分来介绍一下CBOW模型。继续借用上一部分说到的句子中的窗口的概念，在CBOW模型中我们假设窗口的大小$n$为奇数（3,5,7,9等）。在Bengio的工作中他希望利用前$n-1$个词来建模第n个词。而在CBOW模型中我们希望利用周围的词（称为上下文（Context））来对中间词进行建模。例如<code>I love you</code>在这个窗口中可以通过I和you来对中间词love来建模，我们希望$P\left(love|I,you\right)$这个概率比其他词的概率如$P\left(school|I,you\right)$要大。<br>$$P\left(love|I,you\right) &gt; P\left(school|I,you\right)$$<br>这样我们的模型即为：<br><img src="http://ww4.sinaimg.cn/mw690/6dfbc26fjw1es563277b4j20af0cp3zc.jpg" alt="Continuous Bag of Words Model"></p>
<p>回到之前Bengio的那个神经网络，虽然模型很简单，实际上在计算的时候复杂度很高，一方面在对每一个窗口，最后一层都有$|V|$个节点，如果在一个非常大的corpus中计算复杂度非常高。另一方面在模型中存在tanh这样的函数，指数函数在计算机中求解是通过Taylor公式展开的。这样的话在做back-propagation迭代求参数的花费就非常高。Mikolov很好地解决了第二个问题，在构建神经网络模型中，我们对隐藏层不再采用非线性映射，而是普通的线性映射。在输出层我们仍然需要通过softmax函数将score转换成概率。<br>定义一下这个网络的一些符号：</p>
<ul>
<li>$w^{\left(i\right)}$：词典中第i个词</li>
<li>$x_i$：第i个词的One-hot vector表示</li>
<li>$W\in R^{n\times |V|}$：输入层到隐藏层的参数矩阵</li>
<li>$u^{\left(i\right)}$：$W$矩阵中的第$i$列，也就是第$i$个词的enbedding后的表示。</li>
<li>$W^{‘}\in R{n\times |V|}$：隐藏层到输出层的权重参数矩阵。</li>
<li>$v^{\left(i\right)}$：$W^{‘}$矩阵中的第$i$列。也就是第$i$个词的输出表示。<br>这样整个的学习过程就是：</li>
<li>1.通过词典生成一系列的one-hot vector：$(x^{\left(i-c\right)},…,x^{\left(i-1\right)},x^{\left(i+1\right)},…,x^{\left(i+c\right)})$</li>
<li>2.通过$W$得到词向量的表示：$u^{\left(j\right)}=W\cdot x^{\left(j\right)}$，j是在一个集合中：$j\in {i-c,…,i-1,i+1,…,i+c}$</li>
<li>3.取所有算出来的词向量的平均数，即隐藏层的值：$h=\frac{u^{\left(i-c\right)}+…+u^{\left(i-1\right)}+u^{\left(i+1\right)}+…+u^{\left(i+c\right)}}{2c}$</li>
<li>4.计算output层的score vector：$z=W^{‘}h$</li>
<li>5.通过softmax函数将所得到的score转换成概率：$\hat{y}=softmax\left(z\right)$</li>
</ul>
<p>这样我们对单个窗口的目标函数即为：<br>\begin{align}<br>argmax_{\theta}~~J=\log~P(w^{\left(i\right)}|w^{\left(i-c\right)},…w^{\left(i-1\right)},w^{\left(i+1\right)},…,w^{\left(i+c\right)})<br>=\log \frac{exp\left(v\left(i\right)^{T}h\right)}{\sum_{j=1}^{|V|}exp \left(v\left(j\right)^{T}h\right)}<br>\end{align}<br>那么整个dataset的目标函数就是把所有窗口的目标函数加一起。不再详述~</p>
<h1 id="4-Skip-Gram_Model">4.Skip-Gram Model</h1><p>Mikolov的paper中同时介绍了CBOW和这个模型，它俩的idea正好相反，在CBOW中我们希望通过上下文来对中心词进行建模，而Skip-Gram中我们希望通过中心词来对上下文进行建模。下图就是Skip-Gram模型：<br><img src="http://ww3.sinaimg.cn/mw690/6dfbc26fjw1es563b150yj20c90e7dg8.jpg" alt="Skip-Gram Model"><br>在这个模型中，对于每一个窗口，我们想最大化$P\left(context|center word\right)$，我们知道，上下文是这个center word左右两边$c$个词。<br>$$argmax_{\theta}~~J=\log~P\left(w^{\left(i-c\right)},…w^{\left(i-1\right)},w^{\left(i+1\right)},…,w^{\left(i+c\right)}|w^{\left(i\right)}\right)$$<br>因此，直接求这个联合概率很难，我们需要做一些假设（Assumption）。在这个模型中我们做了一个非常强的假设：朴素贝叶斯假设，关于什么是Naive Bayes，可以看一下之前的<a href="http://sunshiningjiang.github.io/2015/05/03/From-Linear-Model-to-Generalized-Linear-Model-Part%201" target="_blank" rel="external">博文</a>。通过Naive Bayes Assumption，我们可以将这个目标函数写成：<br>$$argmax_{\theta}~J=\log~\prod_{j=0,j\neq c}^{2c} P\left(w^{\left(i-c+j\right)}|w^{\left(i\right)}\right)$$<br>然后呢，就是用随机梯度下降来求解啦。。。</p>
<p>终于把Part 2给OVER了。。下一篇博客会讲一下SENNA模型，这个模型出自一篇很牛的文章（<a href="http://arxiv.org/abs/1103.0398" target="_blank" rel="external">Natural Language Processing (almost) from Scratch</a>）。它利用一个很聪明的idea，避免了在输出层要计算$|V|$大小的score。。下次再见！</p>
<h1 id="参考文献">参考文献</h1><p>[1] Collobert, Ronan, et al. “Natural language processing (almost) from scratch.” The Journal of Machine Learning Research 12 (2011): 2493-2537.<br>[2] Bengio, Yoshua, et al. “A neural probabilistic language model.” The Journal of Machine Learning Research 3 (2003): 1137-1155.<br>[3] Mikolov, Tomas, et al. “Efficient estimation of word representations in vector space.” arXiv preprint arXiv:1301.3781 (2013).</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-learning/">Deep learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Natural-language-processing/">Natural language processing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Parsing/">Parsing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Word-Embedding/">Word Embedding</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-Word-Embedding-An-Introduction-and-Its-Application-in-Sentence-Parsing-Part-1" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/05/15/Word-Embedding-An-Introduction-and-Its-Application-in-Sentence-Parsing-Part-1/" class="article-date">
  	<time datetime="2015-05-15T10:54:39.000Z" itemprop="datePublished">2015-05-15</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/15/Word-Embedding-An-Introduction-and-Its-Application-in-Sentence-Parsing-Part-1/">Word Embedding:An Introduction and Its Application in Sentence Parsing - Part 1</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近在组里做了一个报告，关于词向量以及用词向量来做句子parsing。Slides用LaTex 的Beamer框架做的，花了很长时间来做= =。slides的下载地址在<a href="http://vdisk.weibo.com/s/z7KwYz-djcF1f/1431688597" target="_blank" rel="external">这里</a>。现在就打算把它整理成一个博文，边写边整理思路。</p>
<h1 id="1-传统的词的表示方法">1.传统的词的表示方法</h1><h2 id="1-1-One-hot_Vector">1.1.One-hot Vector</h2><p>在自然语言的很多任务当中，我们要对corpus中的词进行建模。因此需要对每个词进行表示，传统的表示方法被称为One-hot Vector，它的意思是我对每个词用一个向量来表示，这个向量的长度是语料库（corpus）的词典的大小。比如一句话“<em>I love ShanghaiTech University</em>”。我可以用<code>如下</code>的表示方法：</p>
<blockquote>
<p>“I”=[1,0,0,…,0,0]<br>“love”=[0,0,1,…,0,0]<br>“ShanghaiTech”=[0,0,,…,1,0]<br>“University”=[0,0,,…,0,1]</p>
</blockquote>
<p>这样的好处是对于一个向量，我看它哪一维度是1就可以代表那个词。坏处也很明显：</p>
<ul>
<li>如果在一个很大的语料库下，词典的维数会很高，这样在以后的计算中cost会很高。</li>
<li>它不能抓取词与词之间的语义关系。<h2 id="1-2-基于类别词的表示">1.2.基于类别词的表示</h2>有一类对词的表示就是对其Assign一个Lable或者一个Lable的分布。比如在主题模型中，我们对每一个文档的每一个词通过生成模型来生成它。这个生成模型的过程是：<br>对于每一个文档，</li>
</ul>
<ol>
<li>从一个泊松分布中Sample该文档中词的数量。</li>
<li>从一个Direchlet分布中Sample该文档所属的topic $\theta$</li>
<li>对于这篇文档N个词中的每一个词，<br> a). Sample一个主题$z_n \sim Multinomial \left(\theta\right)$<br> b). 从$p\left(w_n|z_n,\beta \right)$中Sample一个词。$p\left(w_n|z_n,\beta \right)$这个概率分布函数是一个多项式分布，它是指在主题给定条件下产生词$w_n$的概率。</li>
</ol>
<p><img src="http://ww4.sinaimg.cn/mw690/6dfbc26fjw1es563cmzq0j20vl0h4n33.jpg" alt="主题模型的一个例子"></p>
<p>[Blei, David M., Andrew Y. Ng, and Michael I. Jordan. “Latent dirichlet allocation.” JMLR 2003.]<br>在上图中，左边的四种颜色的方框是四个主题（topic），在每个topic下的word服从多项式分布（multinomial）。如何产生这篇文章的所有词呢，我们通过右边“折方图”（表示每一个文档的topic分布），对于该分布中的每一个topic我们来通过$P\left(w_n|z_n,\beta \right)$生成word，由于文档的topic不是固定的（是一个随机变量，服从多项式分布），对于每一个topic我们都来产生words，因此我们最后得到多数的words很大程度上能够对那个概率比较大的topic的一种表示。<br><img src="http://ww3.sinaimg.cn/mw690/6dfbc26fjw1es5633xm2sj20qq0ex76u.jpg" alt="主题模型"><br>[Blei, David M., Andrew Y. Ng, and Michael I. Jordan. “Latent dirichlet allocation.” JMLR 2003.]<br>上图是LDA的图模型的表示，$\alpha$,$\eta$是超参数，在训练整个模型之前要确定好。$\theta$,$Z$,$\beta$是我们希望来做推断（inference）的随机变量。在整个模型中我们只可观察到D个文档，每个文档有$W_d$个词。</p>
<h1 id="2-基于矩阵SVD分解的词向量">2.基于矩阵SVD分解的词向量</h1><p>第二部分我想讨论一下基于矩阵分解的词向量的方法，我们的目的是获得词典中每个词的一个向量表示。那很直接的想法就是我通过语料库来构建一个Co-occurrence矩阵，矩阵的行和列均代表词典的每个词。矩阵中的每一个值是在语料库中相邻词一起出现的次数。下面我们举个例子：<br>我们现在有如下的语料库：</p>
<blockquote>
<p>I love you.<br>I love you.<br>I love you.<br>I like you.<br>I like you.<br>I hate her.</p>
</blockquote>
<p>通过这个语料库我们可以构建这个Co-occurrence矩阵。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  </span><br><span class="line">la = np.linalg</span><br><span class="line">words = [<span class="string">"I"</span>,<span class="string">"love"</span>,<span class="string">"you"</span>,<span class="string">"her"</span>,<span class="string">"hate"</span>,<span class="string">"like"</span>]</span><br><span class="line">X = np.array([[<span class="number">0</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],</span><br><span class="line">			  [<span class="number">3</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">			  [<span class="number">0</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">2</span>],</span><br><span class="line">			  [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">			  [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">			  [<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line">U,s,Vh = la.svd(X,full_matrices = <span class="keyword">False</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(len(words)):</span><br><span class="line">	plt.text(U[i,<span class="number">0</span>],U[i,<span class="number">1</span>],words[i])</span><br><span class="line">	plt.plot(U[i,<span class="number">0</span>],U[i,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>这个矩阵就是代码中的变量$X$。我们去分解后的$U$矩阵的前两列的值作为每一个词向量。可视化看一下结果：<br><img src="http://ww1.sinaimg.cn/mw690/6dfbc26fjw1es5638ftj3j20fs0bsq35.jpg" alt="用SVD方法来学词向量"><br>我们可以看到，很多信息可以被抓取到，比如，word <code>I</code>和word <code>you</code>在几何关系上面很近。当然，由于语料库很少，深层次的语义还没有被挖掘。这样的方法有什么缺陷呢?可想而知，当语料库很大的时候，词典非常长，矩阵就特别大，我们很难对这个这个矩阵进行SVD分解。于是Bengio等人就在2003年提出了基于神经网络的方法来解决这类问题，后续有很多的变种，都是基于目标函数迭代的思路来得到词向量。我们在下一篇博客在讨论。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-learning/">Deep learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Natural-language-processing/">Natural language processing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Parsing/">Parsing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Word-Embedding/">Word Embedding</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-Comparision-between-Logistic-Regression-and-Naive-Bayes" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/05/08/Comparision-between-Logistic-Regression-and-Naive-Bayes/" class="article-date">
  	<time datetime="2015-05-08T02:23:10.000Z" itemprop="datePublished">2015-05-08</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/08/Comparision-between-Logistic-Regression-and-Naive-Bayes/">Comparision between Logistic Regression and Naive Bayes</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在有监督学习算法用做分类问题时，有两种算法在实际中应用广泛，它们分别是Logistic regression和Naive bayes。今天我们讨论一下这两类算法在分类问题上面的异同点，以及它们的优缺点。</p>
<h1 id="1-两类算法介绍">1.两类算法介绍</h1><h2 id="Logistic_Regression">Logistic Regression</h2><p>对于分类问题，我们有一个训练集合N，我们的训练数据是${x^{\left(1\right)},x^{\left(2\right)},…,x^{\left(m\right)}}$，每一个训练数据$x^{\left(i\right)},i \in {1,2,…,n}$有n个特征，$x^{\left(i\right)}$对应的$y^{\left(i\right)}$是一个离散变量，我们考虑简单情况，假设$y^{\left(i\right)}$取binary的离散值，$\theta \in R^{n}$是我们的参数。Logistic regression模型是指我们认为$\theta_i$与$x_i$之间是线性关系，因此我们构建条件概率$P\left(y|x;\theta\right)$，通过将$\sum_{i=1}^{n}\theta_ix_i$映射到0和1之间的概率：<br>$$P\left(y|x;\theta \right)=\frac{1}{1+e^{-\theta^{T}x}}$$</p>
<h2 id="Naive_Bayes">Naive Bayes</h2><p>Naive Bayes模型是说我有一个很强的假设，这个假设是在$y$给定的条件下，$x_i$相互独立，这个假设看上去非常stupid，但是在实际应用（比如垃圾邮件检测）中能够work非常好。这一方面也验证了Occam’s Theory: Simple model is better。继续主题，既然$x_i$相互独立，我们想对测试数据$\hat{x}$进行预测，可以通过如下方法：<br>$$P\left(\hat{y}=k|\hat{x}\right)=\frac{P\left(\hat{x}|\hat{y}=k\right)P\left(\hat{y}=k\right)}{P\left(\hat{x}\right)}$$<br>在这里，$P\left(\hat{x}|\hat{y}=k\right)$可以通过条件独立性写成：<br>$$P\left(\hat{x}|\hat{y}=k\right)=\prod_{i=1}^{n}p\left(\hat{x}_i|\hat{y}=k\right)$$<br>而分母可以通过联合概率写成：<br>$$P\left(\hat{x}\right)=P\left(\hat{x}|\hat{y}=0\right)P\left(\hat{y}=0\right)+P\left(\hat{x}|\hat{y}=1\right)P\left(\hat{y}=1\right)$$<br>然后再将$P\left(\hat{x}|\hat{y}=k\right)$式子代入，即可算出$P\left(\hat{y}=k|\hat{x}\right)$。可是$p\left(\hat{x}_i|\hat{y}=k\right)$怎么获得呢？直接通过训练数据进行counting就好了，counting的理论解释是最大化似然函数，这里不再详述。</p>
<p>好的，下面我们开始分析Logistic Regression和Naive Bayes的异同点。</p>
<h1 id="2-两者的异同点">2.两者的异同点</h1><h2 id="相同点">相同点</h2><ul>
<li>Logistic regression和Naive bayes都是对特征的线性表达$\sum_j\theta_jx_j$，只是区别在于两者所fit的参数不同。</li>
<li>Logistic regression和Naive bayes建模的都是条件概率$P\left(y=k|x\right)$，对所最终求得的不同类的结果有很好的解释性。而不像SVM，神经网络这样解释性不高。</li>
</ul>
<h2 id="不同点">不同点</h2><ul>
<li><p>Logistic regression在有相关性feature上面学习得到的模型在测试数据的performance更好。也就是说，logistic regression在训练时，不管特征之间有没有相关性，它都能找到最优的参数。而在Naive bayes中，由于我们给定特征直接相互独立的严格设定，在有相关性的feature上面学习到的权重同时变大或变小，它们之间的权重不会相互影响。从这方面来说，如果能够在对参数较好地控制，在损失项方面处理的很好的话，Logistic regression相对Naive bayes在应用时更不会限制在特征工程（feature engineering）上面。</p>
</li>
<li><p>Naive bayes的好处是我没有优化参数这一步，通过训练数据我直接得到一个counting table，这些有助于并行化。</p>
</li>
<li>Andrew Ng和Michael Jordan在2001年发了一篇NIPS短文《<a href="http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf" target="_blank" rel="external">On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes</a>》，他们把这两个模型用在各种数据集上面进行测试，最后得到在小数据上面Naive bayes可以取得更好的效果，随着数据的增多、特征维度的增大，Logistic regression的效果更好。这也是因为Naive bayes是生成模型，在有prior的情况下模型能够把数据fit的更好，而Logistic regression属于生成模型，目标驱动化，不去建模联合概率，通过训练数据直接预测输出，因此在数据足够多的情况下能够得到更好一些的效果。</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Logistic-regression/">Logistic regression</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Naive-bayes/">Naive bayes</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-From-Linear-Model-to-Generalized-Linear-Model-Part-2" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/05/03/From-Linear-Model-to-Generalized-Linear-Model-Part-2/" class="article-date">
  	<time datetime="2015-05-03T14:31:33.000Z" itemprop="datePublished">2015-05-03</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/03/From-Linear-Model-to-Generalized-Linear-Model-Part-2/">From Linear Model to Generalized Linear Model-Part 2</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>上次说到用作回归问题的线性模型，这次我们来讨论一下线性模型如何用来做分类。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Generalized-Linear-Model/">Generalized Linear Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linear-Model/">Linear Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/05/03/hello-world/" class="article-date">
  	<time datetime="2015-05-03T12:45:56.389Z" itemprop="datePublished">2015-05-03</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/03/hello-world/">Hello World</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>感谢<a href="http://zhaoyanpeng.github.io/" target="_blank" rel="external">秦风师弟</a>的帮助，以后这里就是新家啦！</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-From-Linear-Model-to-Generalized-Linear-Model-Part 1" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/05/03/From-Linear-Model-to-Generalized-Linear-Model-Part 1/" class="article-date">
  	<time datetime="2015-05-03T09:14:48.000Z" itemprop="datePublished">2015-05-03</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/03/From-Linear-Model-to-Generalized-Linear-Model-Part 1/">From Linear Model to Generalized Linear Model-Part 1</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>前一阵在一个机器学习的qq群里听了一位妹子做关于线性模型的tutorial，我就按着自己的理解把线性模型的资料窜一下。</p>
<h1 id="1_引入">1 引入</h1><p>我们知道，在机器学习模型中最基础的就是线性模型，无论是有计算机科学味道的机器学习书籍（如<a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/" target="_blank" rel="external">PRML</a>）还是统计系味道较重的统计学习书籍（如<a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/" target="_blank" rel="external">ESL</a>）,或者是各位大神（Andrew Ng, Eric Xing…）的机器学习课程都是先从线性模型还是讲起，可见其基础性。本文先从基本的线性模型，到指数家族，最后谈广义线性模型，以及众多广义线性模型的例子。</p>
<h1 id="2_用于回归问题的线性模型">2 用于回归问题的线性模型</h1><h2 id="2-1_模型定义">2.1 模型定义</h2><p>有监督学习是指通过训练数据学到一个model使得其在测试数据上面能够表现的很好。而回归问题中我们想学到的模型就是一个超平面，我们希望这个超平面能够很好地来fit我们的训练数据。假设我们的输入的一个数据是$x$，这里的$x$是一个向量，每一维度代表这个数据的一个属性即$x_i \in R$，而且$x\in R$，输出是$y$，首先分析一下向量$x$的所有可能取值：</p>
<ul>
<li>原本的数据</li>
<li>原始数据的一些变化，比如：log，exp等</li>
<li>平方，三次方等</li>
<li>不同元素之间的关系，如：$x_3 = x_1 \cdot x_2$</li>
</ul>
<p>既然建立起$x$，一个很直观的想法就是我希望$y$与$x$是一个近似的线性关系。因此，我们定义一个近似$y$的线性函数：<br>$$h_\theta \left(x\right) = \sum_{i=0}^{n} \theta_i x_i = \theta^Tx$$<br>其中，$\theta_i$是参数，可以理解成对每一个 $x_i \in R$ 的一个权重，定义了这个模型，我们接下来希望通过$x$和$y$来获得这些参数。因此，我们定义了一个损失函数 $J\left(\theta \right)$，我们希望平均下来对于每一个$x$输出的$y$及其预测的$h\left(x\right)$之间的平均误差最小。<br>$$J\left(\theta \right) = \frac{1}{2} \sum_{i=1}^{m}\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2$$<br>在这个式子中$x^{(i)}$和$y^{(i)}$分别代表第$i$个数据的输入和输出。我们可以看出，其实这个函数就是中学学到的最小二乘模型。既然目标函数出来了，我们就需要用一些优化方面的技术来求解我们的参数。</p>
<h2 id="2-2_求解模型">2.2 求解模型</h2><h3 id="2-2-1_直接求解">2.2.1 直接求解</h3><p>我们可以看出，我们的目标函数是个凸函数，我们把目标函数写成向量的形式，如下：<br>$$J\left(\theta \right) = \frac{1}{2}\left(X\theta - y\right)^T\left(X\theta - y\right)$$<br>这里我们定义$X = $<br>\begin{bmatrix}<br>—(x^{(1)})^T— \\<br>—(x^{(2)})^T— \\<br>:                 \\<br>:               \\<br>—(x^{(m)})^T—<br>\end{bmatrix}<br>m是指训练数据的数量。每一个$x^{(i)}分别指一个数据$<br>\begin{aligned}\end{aligned}<br>$$\bigtriangledown_{\theta} J\left(\theta\right) = X^T\left(X\theta - y\right)$$<br>设偏导数为0，则<br>$$\theta = \left(X^TX\right)^{-1}X^Ty$$<br>这么轻松就得到了$\theta$的值了，得到$\theta$后，我们看一下对于我们的训练数据$X$，我们可以通过下面这个式子得到对应的$\hat{y}$。<br>$$\hat{y} = X \theta = X\left(X^TX\right)^{-1}X^Ty$$<br>我们可以看到，$y$与$\hat{y}$直接是通过一个矩阵来进行映射，我们把矩阵$H=X\left(X^TX\right)^{-1}X^T$这个矩阵称为帽子矩阵（hat matrix）。这里我们停下来分析一下这个式子的几何意义，<br>在一个m维的空间里，$y$就是一个点，所有的$x^{(i)}$ span 出一个平面。我们的目标就是在这个平面上面搜寻出一个点，使得$y$到这个点的距离最小，因此，最好的点就是$y$到平面的最短距离。</p>
<p>由Gauss-Markov定理可得，这里我们得到的$y_{target}$是对$y$的无偏估计。</p>
<blockquote>
<p>Gauss-Markov定理：$\theta$的最小二乘估计在所有线性无偏估计中方差最小。</p>
</blockquote>
<p>life is really easy？我们仔细观察，这里存在两个问题：</p>
<p>（1）在我们得到最后关于$\theta$的表达式中有一项矩阵求逆，所以，当$X^TX$不能求逆(奇异矩阵)时，即使咱们能得到$\theta$的表达式，也是白搭嘛。有哪些情况呢？如：$x_i$和$x_j$之间有线性关系。这时候怎么办呢，我们需要对$\theta$加一些约束条件。后面有说$LASSO$或者$Ridge$  $Regression$（这里就不再是无偏估计了，我们希望用“有偏”来换取较小的均方误差（MSE））<br>（2）即使用了预处理（特征选择等）什么天马行空的方法，这个矩阵能求逆，当每一个$x^{(i)}$的维数很高时，我们发现，即使简单地求一下$X^TX$都很耗时。看来，我们需要找到更快的解决方法，这就是接下来要说基于梯度的方法。</p>
<h3 id="2-2-2_基于梯度的方法">2.2.2 基于梯度的方法</h3><p>基于梯度的方法基本思想是，既然我不能一次性找到最优的solution，那我慢慢找，通过一个学习法则来找，每一步都满足使得目标函数有一定的提高。这样我们的第一个方法就是梯度下降法，初始化一组参数$\theta$，对于一个训练数据，我们通过如下迭代$\theta$:<br>$$\theta_j:=\theta_j - \alpha \frac{\partial}{\partial \theta_j}J\left(\theta\right)$$<br>通过求导可以得到<br>$$\theta_j:=\theta_j + \alpha \left(y^{(i)}-h_\theta\left(x^{(i)}\right)\right)x_j^{(i)}$$<br>由于我们训练样本有$m$个数据，我们迭代的公式即为：<br>$$\theta_j:=\theta_j + \alpha \sum_{i=1}^{m} \left(y^{(i)}-h_\theta\left(x^{(i)}\right)\right)x_j^{(i)}$$</p>
<p>在这个方法中，每次我们更新一次$\theta$，都要穷举所有的训练样本，因此，这类算法被称为批梯度下降（batch gradient descent）。可想而知，这种算法耗时很长，何不如每次仅考虑一个数据来对$\theta_j$进行更新呢？这就是下面的随机梯度下降（stochastic gradient descent）。</p>
<p>循环直至收敛{<br>    对训练集每一个数据进行操作{<br>    $$\theta_j:=\theta_j + \alpha \left(y^{(i)}-h_\theta\left(x^{(i)}\right)\right)x_j^{(i)}$$<br>    }<br>}</p>
<h2 id="2-3_结果的思考">2.3 结果的思考</h2><p>通过上面的求解，我们可以直接或者间接得到$\theta$的值，在原来当我们设计模型的时候，我们希望$y\approx h_{\theta} \left(x\right)$，实际上，他们直接有一个误差$\epsilon$，我想现在换一个角度考虑，我直接对$y$跟$x$的关系进行建模，因此，<br>$$y^{(i)} = h_{\theta} \left(x^{(i)}\right) + \epsilon^{(i)}$$<br>$\epsilon$服从一个分布函数。</p>
<p>在这里有一个结论，<br>如果$\epsilon^{(i)}$之间独立同分布（IID）,并且服从一个均值为0,方差为$\sigma ^{2}$的高斯分布。<br>$$p\left(\epsilon^{(i)} \right) = \frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{\left(\epsilon^{(i)}\right)^{2}}{2 \sigma ^{2}}\right)$$<br>将$\epsilon^{(i)}$代入，我们可以得到，<br>$$p\left(y^{(i)}|x^{(i)};\theta \right) = \frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{\left(y^{(i)} - h_{\theta} \left(x^{(i)}\right)\right)^{2}}{2 \sigma ^{2}}\right)$$<br>因为训练数据中样本之间关系是IID的，我们可以最大化似然估计（MLE），似然函数为：<br>\begin{aligned}<br>L\left(\theta \right) = L\left(\theta;X,\vec{y} \right) = p\left(\vec{y} | X;\theta \right)<br>= \prod_{i=1}^{m}  \frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{\left(y^{(i)} - h_{\theta} \left(x^{(i)}\right)\right)^{2}}{2 \sigma ^{2}}\right)\\<br>\end{aligned}<br>我们想最大化似然函数，这样就意味着最大化$exp$括号里的式子。可以观测到，里面的式子其实就正比于我们之前的损失函数。而这里，我们最后选取的参数$\theta$跟$\sigma$之间没有关系。这个现象比较有趣，我们将在以后讨论广义线性模型时讨论到它。</p>
<h2 id="2-4_更一般化的损失函数">2.4 更一般化的损失函数</h2><p>在最小二乘里面，我们定义损失函数是通过$l_2$范数，我们可以得到最小化它与$y|x \sim Gauss$所得到的最大化似然函数等价。这里我们若把$l_2$范数改为$l_1$范数或者$l_0$范数呢？它们对应的损失函数分别称为：平方损失，绝对损失和0-1损失。这里我们简单地看一下三个损失函数在描述样本集中趋势的意义：<br>平方损失：<br>$$min_{\theta} ||X-\theta||^2$$<br>绝对损失：<br>$$min_{\theta} |X-\theta|$$<br>0-1损失：<br>$$min_{\theta} 1\left(X \neq\theta \right)$$<br>我们可以看到，在描述样本时，其实它们分别表示：平均数，中位数和众数。<br>另外，在$l_1$范数的损失函数的情况下，最小化损失函数其实就是最大化Laplace分布下的似然函数：<br>$$max_{\beta} \prod_{i} \frac{m}{2} exp \left(-m \sum_i|y_i-x_i^T\beta|\right)$$</p>
<h2 id="2-5_带有惩罚项的目标函数">2.5 带有惩罚项的目标函数</h2><p>在我们做回归问题的时候，为了使得我们的模型有着较强的表达能力，我们建立的$x$包含了原本数据的1-9次方，这样通过构建损失函数学出来的模型在未来测试数据上很大可能会过拟合（overfit）。这时候我们希望对参数$\theta$加一些约束，如$l_1$，$l_2$ 范数。这种方法在统计系被称为Shrinkage方法，当然，引入它的理由不仅仅是为了防止过拟合，很多时候保留或者舍弃一些feature可以使得模型更直观（intuitive），这里我们讨论两种模型，岭回归（Ridge Regression）和LASSO。</p>
<h3 id="2-5-1_Ridge_Regression">2.5.1 Ridge Regression</h3><p>Ridge Regression是指我在目标函数里面加上参数的$l_2$范数，这里我们的目标函数就变成：<br>$$J\left(\theta \right) = \frac{1}{2} \sum_{i=1}^{m}\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2 + \lambda||\theta||_2$$<br>因此，我们得到<br>$$\theta = \left(X^TX+\lambda I\right)^{-1}X^Ty$$</p>
<h3 id="2-5-2_LASSO">2.5.2 LASSO</h3><p>LASSO是指我在目标函数里面加上参数的$l_1$范数<br>$$J\left(\theta \right) = \frac{1}{2} \sum_{i=1}^{m}\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2 + \lambda|\theta|_1$$<br>如何求解呢？<br>求次梯度啊，MP，OMP。。等等。上学期压缩感知学了一学期的这方面的算法和应用。To be writed!</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Generalized-Linear-Model/">Generalized Linear Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linear-Model/">Linear Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-Some-Machine-Learning-Blogs" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/04/19/Some-Machine-Learning-Blogs/" class="article-date">
  	<time datetime="2015-04-19T13:38:31.000Z" itemprop="datePublished">2015-04-19</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/04/19/Some-Machine-Learning-Blogs/">Some Machine Learning Blogs</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="http://quantombone.blogspot.com/" target="_blank" rel="external">Blog by Tomasz Malisiewicz</a>(Phd@CMU Now PostDoc@MIT)<br><a href="https://saravananthirumuruganathan.wordpress.com/category/data-mining/" target="_blank" rel="external">Blog by  Saravanan Thirumuruganathan</a><br><a href="https://onionesquereality.wordpress.com/" target="_blank" rel="external">Blog by Shubhendu Trivedi</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/blog/">blog</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-Math-Resourses" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/04/12/Math-Resourses/" class="article-date">
  	<time datetime="2015-04-12T12:46:57.000Z" itemprop="datePublished">2015-04-12</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/04/12/Math-Resourses/">Math Resourses</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Linear_Algebra">Linear Algebra</h1><p><a href="http://cs229.stanford.edu/section/cs229-linalg.pdf" target="_blank" rel="external">Linear Algebra Review and Reference</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-Resources-of-machine-learning" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/04/05/Resources-of-machine-learning/" class="article-date">
  	<time datetime="2015-04-05T11:10:15.000Z" itemprop="datePublished">2015-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/04/05/Resources-of-machine-learning/">Resources of machine learning</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>下面是搜集的一些机器学习的课程资源：</p>
<h2 id="1-_General_Machine_learning_courses">1. General Machine learning courses</h2><h3 id="1-1_Introduction_to_Machine_Learning_courses">1.1 Introduction to Machine Learning courses</h3><p>Coursera上 Andrew Ng的<a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="external">machine learning</a>课，现在是self learning，可以随时学习。</p>
<h3 id="1-2_Machine_Learning_courses_@_Universities_with_videos">1.2 Machine Learning courses @ Universities with videos</h3><p>Andrew Ng在Stanford开设的<a href="http://cs229.stanford.edu/" target="_blank" rel="external">CS 229: Machine Learning</a>课程，notes很好，多看几遍收获很大。<br>来自Washington的Pedro Domingos在Coursera上面开设的<a href="https://www.coursera.org/course/machlearning" target="_blank" rel="external">Machine Learning</a>，Domingos大牛可谓机器学习界风云人物，发明了Sum-Product Network, Markov Logic等等模型，我等渣渣只能在大牛的脚下做一些小的improvement。<br>CMU院士Tom Mitchell开设的<a href="http://www.cs.cmu.edu/~tom/10701_sp11/" target="_blank" rel="external">10-701/15-781</a></p>
<h3 id="1-3_Machine_Learning_courses_@_Universities_without_videos">1.3 Machine Learning courses @ Universities without videos</h3><p>MIT 6.867 研究生课程<a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/" target="_blank" rel="external">Machine Learning</a>，notes很详细！</p>
<h2 id="2-_Advanced_machine_learning_courses">2. Advanced machine learning courses</h2><p>高级机器学习课程，包括图模型，无参贝叶斯，统计机器学习理论</p>
<h3 id="2-1_Probabilistic_Graphical_Models">2.1 Probabilistic Graphical Models</h3><p>来自CMU的Eric Xing在2014年春季开设的概率图模型：<a href="http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html" target="_blank" rel="external">Probabilistic Graphical Models<br>(Spring 2014)</a></p>
<p>Stanford的Daphne Koller在coursera开设的<a href="https://www.coursera.org/course/pgm" target="_blank" rel="external">Probabilistic Graphical Models</a></p>
<h3 id="2-2_Statistical_Learning_Theory">2.2 Statistical Learning Theory</h3><p>CMU大牛Larry Wasserman的统计机器学习<a href="http://www.stat.cmu.edu/~larry/=sml/" target="_blank" rel="external">Statistical Machine Learning</a></p>
<h3 id="2-3_一些比较老的资源">2.3 一些比较老的资源</h3><h4 id="2-3-1_Copied_from_Dr-_Tomasz-">2.3.1 Copied from Dr. Tomasz.</h4><p>以下资源都是从<a href="http://www.cs.cmu.edu/~tmalisie/mllinks.html" target="_blank" rel="external">Dr. Tomasz</a>主页分享而获得的，<br><strong>With videos</strong><br><a href="http://helper.ipam.ucla.edu/schedule.aspx?pc=gss2005" target="_blank" rel="external">Graduate Summer School: Intelligent Extraction of Information from Graphs and High Dimensional Data.</a><br>UCLA Institute for Pure &amp; Applied Mathematics.<br>July 2005<br>(He highly recomment the Michael Jordan Graphical Model videos!!!)</p>
<p><a href="http://www.msri.org/workshops/298" target="_blank" rel="external">Emphasis Week on Learning and Inference in Vision</a><br>February 2005<br>Simoncelli, Mumford, Fitzgibbon, Efros, Frey, Zhu,<br>Freeman, Black, Blake, Isard, Weiss, Huttenlocher,<br>Yuille, Zabih, Besag, Gottardo, Donoho<br>MSRI</p>
<p><strong>With notes</strong><br><a href="http://www.cs.berkeley.edu/~jordan/courses/281B-spring04/" target="_blank" rel="external">CS 281B / Stat 241B: Statistical Learning Theory</a><br>Spring 2004<br>Michael Jordan<br>Berkeley</p>
<p><a href="http://www.cs.berkeley.edu/~jordan/courses/281A-fall04/" target="_blank" rel="external">CS 281A / Stat 241A: Statistical Learning Theory</a><br>Fall 2004<br>Michael Jordan<br>Berkeley</p>
<p><a href="http://www.mit.edu/~9.520/fall14/" target="_blank" rel="external">9.520: Statistical Learning Theory and Applications</a><br>Spring 2014<br>Tomas Poggio et al<br>MIT<br><a href="http://www.mit.edu/afs/athena/course/9/9.520/www/spring03/" target="_blank" rel="external">Spring 2003</a></p>
<p><a href="http://web.stanford.edu/class/stats315b/" target="_blank" rel="external">Statistics 315B: Modern Applied Statistics: </a><br>Elements of Statistical Learning II<br>Jerome H. Friedman<br>Stanford</p>
<p><a href="http://www.cs.ubc.ca/~murphyk/teaching/cs532c_fall04/index.html" target="_blank" rel="external">Probabilistic Graphical Models</a><br>Fall 2004<br>Kevin Murphy<br>UBC</p>
<h4 id="2-3-2_Some_Graduate_school_videos_and_old_workshops-">2.3.2 Some Graduate school videos and old workshops.</h4><p><strong>With videos</strong><br><a href="http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/DeepLearningWorkshopNIPS2007" target="_blank" rel="external">Deep Learning Workshop: Foundations and Future Directions</a><br><a href="http://courses.cs.washington.edu/courses/cse577/04sp/contents.html#BP" target="_blank" rel="external">Special Topics in Computer Vision</a>04 Spring</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/learning/">learning</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2015 Yong Jiang
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/mobile.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>





<! -- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



  </div>
</body>
</html>