<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>SunShining</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="A blog of a machine learning learner @ShanghaiTech University">
<meta property="og:type" content="website">
<meta property="og:title" content="SunShining">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="SunShining">
<meta property="og:description" content="A blog of a machine learning learner @ShanghaiTech University">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SunShining">
<meta name="twitter:description" content="A blog of a machine learning learner @ShanghaiTech University">
  
    <link rel="alternative" href="/atom.xml" title="SunShining" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  <script src="http://libs.baidu.com/jquery/1.9.0/jquery.js"></script>
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img lazy-src="http://ww1.sinaimg.cn/mw690/6dfbc26fjw1erww25vfwbj208709ddge.jpg" class="js-avatar">
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Yong Jiang</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						<li>友情链接</li>
						
						
						<li>关于我</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="/#" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="http://weibo.com/u/1845215855" title="weibo">weibo</a>
					        
								<a class="rss" target="_blank" href="/#" title="rss">rss</a>
					        
								<a class="zhihu" target="_blank" href="/#" title="zhihu">zhihu</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Deep-learning/" style="font-size: 10px;">Deep learning</a><a href="/tags/Expectation-maximization/" style="font-size: 10px;">Expectation maximization</a><a href="/tags/Generalized-Linear-Model/" style="font-size: 15px;">Generalized Linear Model</a><a href="/tags/Linear-Model/" style="font-size: 15px;">Linear Model</a><a href="/tags/Logistic-regression/" style="font-size: 10px;">Logistic regression</a><a href="/tags/Machine-learning/" style="font-size: 20px;">Machine learning</a><a href="/tags/Naive-bayes/" style="font-size: 10px;">Naive bayes</a><a href="/tags/Natural-language-processing/" style="font-size: 10px;">Natural language processing</a><a href="/tags/PRML/" style="font-size: 10px;">PRML</a><a href="/tags/Parsing/" style="font-size: 10px;">Parsing</a><a href="/tags/Word-Embedding/" style="font-size: 10px;">Word Embedding</a><a href="/tags/blog/" style="font-size: 10px;">blog</a><a href="/tags/hidden-variable/" style="font-size: 10px;">hidden variable</a><a href="/tags/learning/" style="font-size: 10px;">learning</a>
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="http://frankchu0229.github.io/">Frank Chu&#39;s blog</a>
			        
			          <a target="_blank" class="main-nav-link switch-friends-link" href="http://zhaoyanpeng.github.io/">小小小师弟</a>
			        
			        </div>
				</section>
				

				
				
				<section class="switch-part switch-part4">
				
					<div id="js-aboutme">Now I am a graduate student in ShanghaiTech University. I am doing some research projects about Machine learning.</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Yong Jiang</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="http://ww1.sinaimg.cn/mw690/6dfbc26fjw1erww25vfwbj208709ddge.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">Yong Jiang</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="/#" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="http://weibo.com/u/1845215855" title="weibo">weibo</a>
			        
						<a class="rss" target="_blank" href="/#" title="rss">rss</a>
			        
						<a class="zhihu" target="_blank" href="/#" title="zhihu">zhihu</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-Word-Embedding-An-Introduction-and-Its-Application-in-Sentence-Parsing" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/05/15/Word-Embedding-An-Introduction-and-Its-Application-in-Sentence-Parsing/" class="article-date">
  	<time datetime="2015-05-15T10:54:39.000Z" itemprop="datePublished">2015-05-15</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/15/Word-Embedding-An-Introduction-and-Its-Application-in-Sentence-Parsing/">Word Embedding:An Introduction and Its Application in Sentence Parsing</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近在组里做了一个报告，关于词向量以及用词向量来做句子parsing的topic。Slides用LaTex的Beamer框架做的，花了很长时间来做= =。slides的下载地址在<a href="http://vdisk.weibo.com/s/z7KwYz-djcF1f/1431688597" target="_blank" rel="external">这里</a>。现在就打算把它整理成一个博文，边写边整理思路。</p>
<h1 id="1-传统的词的表示方法">1.传统的词的表示方法</h1><p>在自然语言的很多任务当中，我们要对corpus中的词进行建模。因此需要对每个词进行表示，传统的表示方法被称为One-hot Vector，它的意思是我对每个词用一个向量来表示，这个向量的长度是语料库（corpus）的词典的大小。比如一句话“I love ShanghaiTech University”。我可以用如下的表示方法：</p>
<blockquote>
<p>“I”=[1,0,0,…,0,0]<br>“love”=[0,0,1,…,0,0]<br>“ShanghaiTech”=[0,0,,…,1,0]<br>“University”=[0,0,,…,0,1]</p>
</blockquote>
<h1 id="2-基于矩阵SVD分解的词向量">2.基于矩阵SVD分解的词向量</h1><h1 id="3-基于目标函数迭代的词向量:模型表示">3.基于目标函数迭代的词向量:模型表示</h1><h1 id="4-基于目标函数迭代的词向量:模型学习">4.基于目标函数迭代的词向量:模型学习</h1><h1 id="5-利用词向量来做句子解析">5.利用词向量来做句子解析</h1>
      
    </div>
    
    <div class="article-info article-info-index">
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-learning/">Deep learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Natural-language-processing/">Natural language processing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Parsing/">Parsing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Word-Embedding/">Word Embedding</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-An-Introduction-to-EM-Algorithm" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/05/08/An-Introduction-to-EM-Algorithm/" class="article-date">
  	<time datetime="2015-05-08T09:43:29.000Z" itemprop="datePublished">2015-05-08</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/08/An-Introduction-to-EM-Algorithm/">An Introduction to EM Algorithm</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>机器学习主要研究如何搭建模型和如何求解模型得到参数，我们知道，在有监督学习的问题当中，模型中所有的变量（variable）都是observed的，我们可以通过变量之间的关系构建似然函数（likelihood），然后最大化似然函数（maximize likelihood）得到参数。比如，在分类问题中，模型中的变量（variable）是输入$X$以及输出$Y$，我们可以构建如下case 1的图模型用来表示，这里Y是离散值。我们可以认为每一类都服从协方差相同的Gaussion分布，这样对整个数据的联合概率进行建模的机器学习模型被称为高斯判别分析（Gaussion Discriminant Analysis）。<br><img src="http://ww4.sinaimg.cn/mw690/6dfbc26fjw1erx14zl3ayj210e0t2779.jpg" alt="图模型表示"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Expectation-maximization/">Expectation maximization</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hidden-variable/">hidden variable</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-Comparision-between-Logistic-Regression-and-Naive-Bayes" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/05/08/Comparision-between-Logistic-Regression-and-Naive-Bayes/" class="article-date">
  	<time datetime="2015-05-08T02:23:10.000Z" itemprop="datePublished">2015-05-08</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/08/Comparision-between-Logistic-Regression-and-Naive-Bayes/">Comparision between Logistic Regression and Naive Bayes</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在有监督学习算法用做分类问题时，有两种算法在实际中应用广泛，它们分别是Logistic regression和Naive bayes。今天我们讨论一下这两类算法在分类问题上面的异同点，以及它们的优缺点。</p>
<h1 id="1-两类算法介绍">1.两类算法介绍</h1><h2 id="Logistic_Regression">Logistic Regression</h2><p>对于分类问题，我们有一个训练集合N，我们的训练数据是${x^{\left(1\right)},x^{\left(2\right)},…,x^{\left(m\right)}}$，每一个训练数据$x^{\left(i\right)},i \in {1,2,…,n}$有n个特征，$x^{\left(i\right)}$对应的$y^{\left(i\right)}$是一个离散变量，我们考虑简单情况，假设$y^{\left(i\right)}$取binary的离散值，$\theta \in R^{n}$是我们的参数。Logistic regression模型是指我们认为$\theta_i$与$x_i$之间是线性关系，因此我们构建条件概率$P\left(y|x;\theta\right)$，通过将$\sum_{i=1}^{n}\theta_ix_i$映射到0和1之间的概率：<br>$$P\left(y|x;\theta \right)=\frac{1}{1+e^{-\theta^{T}x}}$$</p>
<h2 id="Naive_Bayes">Naive Bayes</h2><p>Naive Bayes模型是说我有一个很强的假设，这个假设是在$y$给定的条件下，$x_i$相互独立，这个假设看上去非常stupid，但是在实际应用（比如垃圾邮件检测）中能够work非常好。这一方面也验证了Occam’s Theory: Simple model is better。继续主题，既然$x_i$相互独立，我们想对测试数据$\hat{x}$进行预测，可以通过如下方法：<br>$$P\left(\hat{y}=k|\hat{x}\right)=\frac{P\left(\hat{x}|\hat{y}=k\right)P\left(\hat{y}=k\right)}{P\left(\hat{x}\right)}$$<br>在这里，$P\left(\hat{x}|\hat{y}=k\right)$可以通过条件独立性写成：<br>$$P\left(\hat{x}|\hat{y}=k\right)=\prod_{i=1}^{n}p\left(\hat{x}_i|\hat{y}=k\right)$$<br>而分母可以通过联合概率写成：<br>$$P\left(\hat{x}\right)=P\left(\hat{x}|\hat{y}=0\right)P\left(\hat{y}=0\right)+P\left(\hat{x}|\hat{y}=1\right)P\left(\hat{y}=1\right)$$<br>然后再将$P\left(\hat{x}|\hat{y}=k\right)$式子代入，即可算出$P\left(\hat{y}=k|\hat{x}\right)$。可是$p\left(\hat{x}_i|\hat{y}=k\right)$怎么获得呢？直接通过训练数据进行counting就好了，counting的理论解释是最大化似然函数，这里不再详述。</p>
<p>好的，下面我们开始分析Logistic Regression和Naive Bayes的异同点。</p>
<h1 id="2-两者的异同点">2.两者的异同点</h1><h2 id="相同点">相同点</h2><ul>
<li>Logistic regression和Naive bayes都是对特征的线性表达$\sum_j\theta_jx_j$，只是区别在于两者所fit的参数不同。</li>
<li>Logistic regression和Naive bayes建模的都是条件概率$P\left(y=k|x\right)$，对所最终求得的不同类的结果有很好的解释性。而不像SVM，神经网络这样解释性不高。</li>
</ul>
<h2 id="不同点">不同点</h2><ul>
<li><p>Logistic regression在有相关性feature上面学习得到的模型在测试数据的performance更好。也就是说，logistic regression在训练时，不管特征之间有没有相关性，它都能找到最优的参数。而在Naive bayes中，由于我们给定特征直接相互独立的严格设定，在有相关性的feature上面学习到的权重同时变大或变小，它们之间的权重不会相互影响。从这方面来说，如果能够在对参数较好地控制，在损失项方面处理的很好的话，Logistic regression相对Naive bayes在应用时更不会限制在特征工程（feature engineering）上面。</p>
</li>
<li><p>Naive bayes的好处是我没有优化参数这一步，通过训练数据我直接得到一个counting table，这些有助于并行化。</p>
</li>
<li>Andrew Ng和Michael Jordan在2001年发了一篇NIPS短文《<a href="http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf" target="_blank" rel="external">On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes</a>》，他们把这两个模型用在各种数据集上面进行测试，最后得到在小数据上面Naive bayes可以取得更好的效果，随着数据的增多、特征维度的增大，Logistic regression的效果更好。这也是因为Naive bayes是生成模型，在有prior的情况下模型能够把数据fit的更好，而Logistic regression属于生成模型，目标驱动化，不去建模联合概率，通过训练数据直接预测输出，因此在数据足够多的情况下能够得到更好一些的效果。</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Logistic-regression/">Logistic regression</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Naive-bayes/">Naive bayes</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-From-Linear-Model-to-Generalized-Linear-Model-Part-2" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/05/03/From-Linear-Model-to-Generalized-Linear-Model-Part-2/" class="article-date">
  	<time datetime="2015-05-03T14:31:33.000Z" itemprop="datePublished">2015-05-03</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/03/From-Linear-Model-to-Generalized-Linear-Model-Part-2/">From Linear Model to Generalized Linear Model-Part 2</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>上次说到用作回归问题的线性模型，这次我们来讨论一下线性模型如何用来做分类。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Generalized-Linear-Model/">Generalized Linear Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linear-Model/">Linear Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/05/03/hello-world/" class="article-date">
  	<time datetime="2015-05-03T12:45:56.389Z" itemprop="datePublished">2015-05-03</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/03/hello-world/">Hello World</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>感谢<a href="http://zhaoyanpeng.github.io/" target="_blank" rel="external">秦风师弟</a>的帮助，以后这里就是新家啦！</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-From-Linear-Model-to-Generalized-Linear-Model-Part 1" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/05/03/From-Linear-Model-to-Generalized-Linear-Model-Part 1/" class="article-date">
  	<time datetime="2015-05-03T09:14:48.000Z" itemprop="datePublished">2015-05-03</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/03/From-Linear-Model-to-Generalized-Linear-Model-Part 1/">From Linear Model to Generalized Linear Model-Part 1</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>前一阵在一个机器学习的qq群里听了一位妹子做关于线性模型的tutorial，我就按着自己的理解把线性模型的资料窜一下。</p>
<h1 id="1_引入">1 引入</h1><p>我们知道，在机器学习模型中最基础的就是线性模型，无论是有计算机科学味道的机器学习书籍（如<a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/" target="_blank" rel="external">PRML</a>）还是统计系味道较重的统计学习书籍（如<a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/" target="_blank" rel="external">ESL</a>）,或者是各位大神（Andrew Ng, Eric Xing…）的机器学习课程都是先从线性模型还是讲起，可见其基础性。本文先从基本的线性模型，到指数家族，最后谈广义线性模型，以及众多广义线性模型的例子。</p>
<h1 id="2_用于回归问题的线性模型">2 用于回归问题的线性模型</h1><h2 id="2-1_模型定义">2.1 模型定义</h2><p>有监督学习是指通过训练数据学到一个model使得其在测试数据上面能够表现的很好。而回归问题中我们想学到的模型就是一个超平面，我们希望这个超平面能够很好地来fit我们的训练数据。假设我们的输入的一个数据是$x$，这里的$x$是一个向量，每一维度代表这个数据的一个属性即$x_i \in R$，而且$x\in R$，输出是$y$，首先分析一下向量$x$的所有可能取值：</p>
<ul>
<li>原本的数据</li>
<li>原始数据的一些变化，比如：log，exp等</li>
<li>平方，三次方等</li>
<li>不同元素之间的关系，如：$x_3 = x_1 \cdot x_2$</li>
</ul>
<p>既然建立起$x$，一个很直观的想法就是我希望$y$与$x$是一个近似的线性关系。因此，我们定义一个近似$y$的线性函数：<br>$$h_\theta \left(x\right) = \sum_{i=0}^{n} \theta_i x_i = \theta^Tx$$<br>其中，$\theta_i$是参数，可以理解成对每一个 $x_i \in R$ 的一个权重，定义了这个模型，我们接下来希望通过$x$和$y$来获得这些参数。因此，我们定义了一个损失函数 $J\left(\theta \right)$，我们希望平均下来对于每一个$x$输出的$y$及其预测的$h\left(x\right)$之间的平均误差最小。<br>$$J\left(\theta \right) = \frac{1}{2} \sum_{i=1}^{m}\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2$$<br>在这个式子中$x^{(i)}$和$y^{(i)}$分别代表第$i$个数据的输入和输出。我们可以看出，其实这个函数就是中学学到的最小二乘模型。既然目标函数出来了，我们就需要用一些优化方面的技术来求解我们的参数。</p>
<h2 id="2-2_求解模型">2.2 求解模型</h2><h3 id="2-2-1_直接求解">2.2.1 直接求解</h3><p>我们可以看出，我们的目标函数是个凸函数，我们把目标函数写成向量的形式，如下：<br>$$J\left(\theta \right) = \frac{1}{2}\left(X\theta - y\right)^T\left(X\theta - y\right)$$<br>这里我们定义$X = $<br>\begin{bmatrix}<br>—(x^{(1)})^T— \\<br>—(x^{(2)})^T— \\<br>:                 \\<br>:               \\<br>—(x^{(m)})^T—<br>\end{bmatrix}<br>m是指训练数据的数量。每一个$x^{(i)}分别指一个数据$<br>\begin{aligned}\end{aligned}<br>$$\bigtriangledown_{\theta} J\left(\theta\right) = X^T\left(X\theta - y\right)$$<br>设偏导数为0，则<br>$$\theta = \left(X^TX\right)^{-1}X^Ty$$<br>这么轻松就得到了$\theta$的值了，得到$\theta$后，我们看一下对于我们的训练数据$X$，我们可以通过下面这个式子得到对应的$\hat{y}$。<br>$$\hat{y} = X \theta = X\left(X^TX\right)^{-1}X^Ty$$<br>我们可以看到，$y$与$\hat{y}$直接是通过一个矩阵来进行映射，我们把矩阵$H=X\left(X^TX\right)^{-1}X^T$这个矩阵称为帽子矩阵（hat matrix）。这里我们停下来分析一下这个式子的几何意义，<br>在一个m维的空间里，$y$就是一个点，所有的$x^{(i)}$ span 出一个平面。我们的目标就是在这个平面上面搜寻出一个点，使得$y$到这个点的距离最小，因此，最好的点就是$y$到平面的最短距离。</p>
<p>由Gauss-Markov定理可得，这里我们得到的$y_{target}$是对$y$的无偏估计。</p>
<blockquote>
<p>Gauss-Markov定理：$\theta$的最小二乘估计在所有线性无偏估计中方差最小。</p>
</blockquote>
<p>life is really easy？我们仔细观察，这里存在两个问题：</p>
<p>（1）在我们得到最后关于$\theta$的表达式中有一项矩阵求逆，所以，当$X^TX$不能求逆(奇异矩阵)时，即使咱们能得到$\theta$的表达式，也是白搭嘛。有哪些情况呢？如：$x_i$和$x_j$之间有线性关系。这时候怎么办呢，我们需要对$\theta$加一些约束条件。后面有说$LASSO$或者$Ridge$  $Regression$（这里就不再是无偏估计了，我们希望用“有偏”来换取较小的均方误差（MSE））<br>（2）即使用了预处理（特征选择等）什么天马行空的方法，这个矩阵能求逆，当每一个$x^{(i)}$的维数很高时，我们发现，即使简单地求一下$X^TX$都很耗时。看来，我们需要找到更快的解决方法，这就是接下来要说基于梯度的方法。</p>
<h3 id="2-2-2_基于梯度的方法">2.2.2 基于梯度的方法</h3><p>基于梯度的方法基本思想是，既然我不能一次性找到最优的solution，那我慢慢找，通过一个学习法则来找，每一步都满足使得目标函数有一定的提高。这样我们的第一个方法就是梯度下降法，初始化一组参数$\theta$，对于一个训练数据，我们通过如下迭代$\theta$:<br>$$\theta_j:=\theta_j - \alpha \frac{\partial}{\partial \theta_j}J\left(\theta\right)$$<br>通过求导可以得到<br>$$\theta_j:=\theta_j + \alpha \left(y^{(i)}-h_\theta\left(x^{(i)}\right)\right)x_j^{(i)}$$<br>由于我们训练样本有$m$个数据，我们迭代的公式即为：<br>$$\theta_j:=\theta_j + \alpha \sum_{i=1}^{m} \left(y^{(i)}-h_\theta\left(x^{(i)}\right)\right)x_j^{(i)}$$</p>
<p>在这个方法中，每次我们更新一次$\theta$，都要穷举所有的训练样本，因此，这类算法被称为批梯度下降（batch gradient descent）。可想而知，这种算法耗时很长，何不如每次仅考虑一个数据来对$\theta_j$进行更新呢？这就是下面的随机梯度下降（stochastic gradient descent）。</p>
<p>循环直至收敛{<br>    对训练集每一个数据进行操作{<br>    $$\theta_j:=\theta_j + \alpha \left(y^{(i)}-h_\theta\left(x^{(i)}\right)\right)x_j^{(i)}$$<br>    }<br>}</p>
<h2 id="2-3_结果的思考">2.3 结果的思考</h2><p>通过上面的求解，我们可以直接或者间接得到$\theta$的值，在原来当我们设计模型的时候，我们希望$y\approx h_{\theta} \left(x\right)$，实际上，他们直接有一个误差$\epsilon$，我想现在换一个角度考虑，我直接对$y$跟$x$的关系进行建模，因此，<br>$$y^{(i)} = h_{\theta} \left(x^{(i)}\right) + \epsilon^{(i)}$$<br>$\epsilon$服从一个分布函数。</p>
<p>在这里有一个结论，<br>如果$\epsilon^{(i)}$之间独立同分布（IID）,并且服从一个均值为0,方差为$\sigma ^{2}$的高斯分布。<br>$$p\left(\epsilon^{(i)} \right) = \frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{\left(\epsilon^{(i)}\right)^{2}}{2 \sigma ^{2}}\right)$$<br>将$\epsilon^{(i)}$代入，我们可以得到，<br>$$p\left(y^{(i)}|x^{(i)};\theta \right) = \frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{\left(y^{(i)} - h_{\theta} \left(x^{(i)}\right)\right)^{2}}{2 \sigma ^{2}}\right)$$<br>因为训练数据中样本之间关系是IID的，我们可以最大化似然估计（MLE），似然函数为：<br>\begin{aligned}<br>L\left(\theta \right) = L\left(\theta;X,\vec{y} \right) = p\left(\vec{y} | X;\theta \right)<br>= \prod_{i=1}^{m}  \frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{\left(y^{(i)} - h_{\theta} \left(x^{(i)}\right)\right)^{2}}{2 \sigma ^{2}}\right)\\<br>\end{aligned}<br>我们想最大化似然函数，这样就意味着最大化$exp$括号里的式子。可以观测到，里面的式子其实就正比于我们之前的损失函数。而这里，我们最后选取的参数$\theta$跟$\sigma$之间没有关系。这个现象比较有趣，我们将在以后讨论广义线性模型时讨论到它。</p>
<h2 id="2-4_更一般化的损失函数">2.4 更一般化的损失函数</h2><p>在最小二乘里面，我们定义损失函数是通过$l_2$范数，我们可以得到最小化它与$y|x \sim Gauss$所得到的最大化似然函数等价。这里我们若把$l_2$范数改为$l_1$范数或者$l_0$范数呢？它们对应的损失函数分别称为：平方损失，绝对损失和0-1损失。这里我们简单地看一下三个损失函数在描述样本集中趋势的意义：<br>平方损失：<br>$$min_{\theta} ||X-\theta||^2$$<br>绝对损失：<br>$$min_{\theta} |X-\theta|$$<br>0-1损失：<br>$$min_{\theta} 1\left(X \neq\theta \right)$$<br>我们可以看到，在描述样本时，其实它们分别表示：平均数，中位数和众数。<br>另外，在$l_1$范数的损失函数的情况下，最小化损失函数其实就是最大化Laplace分布下的似然函数：<br>$$max_{\beta} \prod_{i} \frac{m}{2} exp \left(-m \sum_i|y_i-x_i^T\beta|\right)$$</p>
<h2 id="2-5_带有惩罚项的目标函数">2.5 带有惩罚项的目标函数</h2><p>在我们做回归问题的时候，为了使得我们的模型有着较强的表达能力，我们建立的$x$包含了原本数据的1-9次方，这样通过构建损失函数学出来的模型在未来测试数据上很大可能会过拟合（overfit）。这时候我们希望对参数$\theta$加一些约束，如$l_1$，$l_2$ 范数。这种方法在统计系被称为Shrinkage方法，当然，引入它的理由不仅仅是为了防止过拟合，很多时候保留或者舍弃一些feature可以使得模型更直观（intuitive），这里我们讨论两种模型，岭回归（Ridge Regression）和LASSO。</p>
<h3 id="2-5-1_Ridge_Regression">2.5.1 Ridge Regression</h3><p>Ridge Regression是指我在目标函数里面加上参数的$l_2$范数，这里我们的目标函数就变成：<br>$$J\left(\theta \right) = \frac{1}{2} \sum_{i=1}^{m}\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2 + \lambda||\theta||_2$$<br>因此，我们得到<br>$$\theta = \left(X^TX+\lambda I\right)^{-1}X^Ty$$</p>
<h3 id="2-5-2_LASSO">2.5.2 LASSO</h3><p>LASSO是指我在目标函数里面加上参数的$l_1$范数<br>$$J\left(\theta \right) = \frac{1}{2} \sum_{i=1}^{m}\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2 + \lambda|\theta|_1$$<br>如何求解呢？<br>求次梯度啊，MP，OMP。。等等。上学期压缩感知学了一学期的这方面的算法和应用。To be writed!</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Generalized-Linear-Model/">Generalized Linear Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linear-Model/">Linear Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-Some-Machine-Learning-Blogs" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/04/19/Some-Machine-Learning-Blogs/" class="article-date">
  	<time datetime="2015-04-19T13:38:31.000Z" itemprop="datePublished">2015-04-19</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/04/19/Some-Machine-Learning-Blogs/">Some Machine Learning Blogs</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="http://quantombone.blogspot.com/" target="_blank" rel="external">Blog by Tomasz Malisiewicz</a>(Phd@CMU Now PostDoc@MIT)<br><a href="https://saravananthirumuruganathan.wordpress.com/category/data-mining/" target="_blank" rel="external">Blog by  Saravanan Thirumuruganathan</a><br><a href="https://onionesquereality.wordpress.com/" target="_blank" rel="external">Blog by Shubhendu Trivedi</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/blog/">blog</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-Math-Resourses" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/04/12/Math-Resourses/" class="article-date">
  	<time datetime="2015-04-12T12:46:57.000Z" itemprop="datePublished">2015-04-12</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/04/12/Math-Resourses/">Math Resourses</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Linear_Algebra">Linear Algebra</h1><p><a href="http://cs229.stanford.edu/section/cs229-linalg.pdf" target="_blank" rel="external">Linear Algebra Review and Reference</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-PRML-reading-notes" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/04/05/PRML-reading-notes/" class="article-date">
  	<time datetime="2015-04-05T13:50:52.000Z" itemprop="datePublished">2015-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/04/05/PRML-reading-notes/">PRML reading notes</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>PRML(Pattern Recognition and Machine Learning)这本书正式开始阅读是从去年九月份。从一开始看的晕晕乎乎，到现在能够看进去，这中间也走了不少弯路。现在重新读一遍，跟随着Toronto大学的<a href="http://www.cs.toronto.edu/~rsalakhu/STA4273_2015/" target="_blank" rel="external">统计机器学习</a>课程，以此blog记录一下阅读笔记。</p>
<h2 id="ch1_Introduction">ch1 Introduction</h2><p><img src="http://ww1.sinaimg.cn/mw690/6dfbc26fjw1eqv29oa539j20nb0ebgqv.jpg" alt="Machine learning: a big picture"><br>机器学习主要分为有监督的学习和无监督学习，它们之间的区别在于target是否给定。在有监督学习中，target给定，若是离散值，比如给一张动物的图片，判断它是小狗还是小猫，这类问题是分类问题（Classification）；若取连续值，比如预测上海某个小区的房价，则是回归问题（Regression）。而在无监督学习中，target不存在，这里有density estimation, clustering, dimensionality reduction, finding hidden explanation等等任务。<br><img src="http://ww2.sinaimg.cn/mw690/6dfbc26fjw1eqv2dl7cxej20y30frq72.jpg" alt="Classification: an example"></p>
<h3 id="ch_1-1_Exmaple:_Polynomial_Curve_Fitting">ch 1.1 Exmaple: Polynomial Curve Fitting</h3><p>我们从最简单的回归问题（曲线拟合）开始说起，Given 一系列训练数据$\left( x^{\left( 1\right) },x^{\left( 2\right) },…,x^{\left( N\right) }\right) ^{T}$, 每一个$x^{i}=x^{i}_1…x^{i}_d$。它们对应的target值$y=(y_1,…,y_N)^{T}$。我们认为$x$与$y$的关系是线性关系，因此，我们对每一个$x^{\left( i \right)}$的预测值<br>$${y(x^{\left( i \right)},w) = \left( x^{\left( i \right)} \right)^{T}w}$$<br>这里我们可以写成<br>$${y(X,w) = X^{T}w}$$<br>注意$X$是一个$N \times \left( d+1 \right)$的矩阵。<br>我们希望通过最小化一个loss function来获得最好的$w$，在这里定义loss function$E\left( w \right)$为squred error，<br>$$E\left( w \right) = \frac{1}{2} (Xw - y)^{T}(Xw - y)$$<br>如果$X^{T}X$是非奇异的矩阵，我们可以求出最优解<br>$$w^{*} = \left( X^{T}X \right)^{-1}X^{T}y$$<br>但是在实际中如果N很大，这里矩阵运算将非常耗时。在matlab下面做了个实验，当N=10000时，只是做$X^{T}X$就要耗时13秒。显然这种方法在大规模实际应用中不可取。<br>另外，这里的linear models是指关于coeffients $w$的线性函数，而不是关于$x$的线性函数。<br>好的，既然已经有了线性模型的解决方法，我们用它来对一个$\sin \left( 2\pi x\right)$进行拟合。我们从这个函数中sampling出来一些data，如下图<br><img src="http://ww2.sinaimg.cn/mw690/6dfbc26fjw1eqvt3fb3v2j20ak07v74e.jpg" alt="$\sin \left( 2\pi x\right)$"><br>我们用一个多项式的函数来拟合这些数据，</p>
<p>TBA…</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PRML/">PRML</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
    <article id="post-Resources-of-machine-learning" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/04/05/Resources-of-machine-learning/" class="article-date">
  	<time datetime="2015-04-05T11:10:15.000Z" itemprop="datePublished">2015-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/04/05/Resources-of-machine-learning/">Resources of machine learning</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>下面是搜集的一些机器学习的课程资源：</p>
<h2 id="1-_General_Machine_learning_courses">1. General Machine learning courses</h2><h3 id="1-1_Introduction_to_Machine_Learning_courses">1.1 Introduction to Machine Learning courses</h3><p>Coursera上 Andrew Ng的<a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="external">machine learning</a>课，现在是self learning，可以随时学习。</p>
<h3 id="1-2_Machine_Learning_courses_@_Universities_with_videos">1.2 Machine Learning courses @ Universities with videos</h3><p>Andrew Ng在Stanford开设的<a href="http://cs229.stanford.edu/" target="_blank" rel="external">CS 229: Machine Learning</a>课程，notes很好，多看几遍收获很大。<br>来自Washington的Pedro Domingos在Coursera上面开设的<a href="https://www.coursera.org/course/machlearning" target="_blank" rel="external">Machine Learning</a>，Domingos大牛可谓机器学习界风云人物，发明了Sum-Product Network, Markov Logic等等模型，我等渣渣只能在大牛的脚下做一些小的improvement。<br>CMU院士Tom Mitchell开设的<a href="http://www.cs.cmu.edu/~tom/10701_sp11/" target="_blank" rel="external">10-701/15-781</a></p>
<h3 id="1-3_Machine_Learning_courses_@_Universities_without_videos">1.3 Machine Learning courses @ Universities without videos</h3><p>MIT 6.867 研究生课程<a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/" target="_blank" rel="external">Machine Learning</a>，notes很详细！</p>
<h2 id="2-_Advanced_machine_learning_courses">2. Advanced machine learning courses</h2><p>高级机器学习课程，包括图模型，无参贝叶斯，统计机器学习理论</p>
<h3 id="2-1_Probabilistic_Graphical_Models">2.1 Probabilistic Graphical Models</h3><p>来自CMU的Eric Xing在2014年春季开设的概率图模型：<a href="http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html" target="_blank" rel="external">Probabilistic Graphical Models<br>(Spring 2014)</a></p>
<p>Stanford的Daphne Koller在coursera开设的<a href="https://www.coursera.org/course/pgm" target="_blank" rel="external">Probabilistic Graphical Models</a></p>
<h3 id="2-2_Statistical_Learning_Theory">2.2 Statistical Learning Theory</h3><p>CMU大牛Larry Wasserman的统计机器学习<a href="http://www.stat.cmu.edu/~larry/=sml/" target="_blank" rel="external">Statistical Machine Learning</a></p>
<h3 id="2-3_一些比较老的资源">2.3 一些比较老的资源</h3><h4 id="2-3-1_Copied_from_Dr-_Tomasz-">2.3.1 Copied from Dr. Tomasz.</h4><p>以下资源都是从<a href="http://www.cs.cmu.edu/~tmalisie/mllinks.html" target="_blank" rel="external">Dr. Tomasz</a>主页分享而获得的，<br><strong>With videos</strong><br><a href="http://helper.ipam.ucla.edu/schedule.aspx?pc=gss2005" target="_blank" rel="external">Graduate Summer School: Intelligent Extraction of Information from Graphs and High Dimensional Data.</a><br>UCLA Institute for Pure &amp; Applied Mathematics.<br>July 2005<br>(He highly recomment the Michael Jordan Graphical Model videos!!!)</p>
<p><a href="http://www.msri.org/workshops/298" target="_blank" rel="external">Emphasis Week on Learning and Inference in Vision</a><br>February 2005<br>Simoncelli, Mumford, Fitzgibbon, Efros, Frey, Zhu,<br>Freeman, Black, Blake, Isard, Weiss, Huttenlocher,<br>Yuille, Zabih, Besag, Gottardo, Donoho<br>MSRI</p>
<p><strong>With notes</strong><br><a href="http://www.cs.berkeley.edu/~jordan/courses/281B-spring04/" target="_blank" rel="external">CS 281B / Stat 241B: Statistical Learning Theory</a><br>Spring 2004<br>Michael Jordan<br>Berkeley</p>
<p><a href="http://www.cs.berkeley.edu/~jordan/courses/281A-fall04/" target="_blank" rel="external">CS 281A / Stat 241A: Statistical Learning Theory</a><br>Fall 2004<br>Michael Jordan<br>Berkeley</p>
<p><a href="http://www.mit.edu/~9.520/fall14/" target="_blank" rel="external">9.520: Statistical Learning Theory and Applications</a><br>Spring 2014<br>Tomas Poggio et al<br>MIT<br><a href="http://www.mit.edu/afs/athena/course/9/9.520/www/spring03/" target="_blank" rel="external">Spring 2003</a></p>
<p><a href="http://web.stanford.edu/class/stats315b/" target="_blank" rel="external">Statistics 315B: Modern Applied Statistics: </a><br>Elements of Statistical Learning II<br>Jerome H. Friedman<br>Stanford</p>
<p><a href="http://www.cs.ubc.ca/~murphyk/teaching/cs532c_fall04/index.html" target="_blank" rel="external">Probabilistic Graphical Models</a><br>Fall 2004<br>Kevin Murphy<br>UBC</p>
<h4 id="2-3-2_Some_Graduate_school_videos_and_old_workshops-">2.3.2 Some Graduate school videos and old workshops.</h4><p><strong>With videos</strong><br><a href="http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/DeepLearningWorkshopNIPS2007" target="_blank" rel="external">Deep Learning Workshop: Foundations and Future Directions</a><br><a href="http://courses.cs.washington.edu/courses/cse577/04sp/contents.html#BP" target="_blank" rel="external">Special Topics in Computer Vision</a>04 Spring</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/learning/">learning</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>






  
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2015 Yong Jiang
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/mobile.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>





<! -- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



  </div>
</body>
</html>