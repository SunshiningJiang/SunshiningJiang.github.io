<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>SunShining</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="A blog of a machine learning learner @ShanghaiTech University">
<meta property="og:type" content="website">
<meta property="og:title" content="SunShining">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="SunShining">
<meta property="og:description" content="A blog of a machine learning learner @ShanghaiTech University">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SunShining">
<meta name="twitter:description" content="A blog of a machine learning learner @ShanghaiTech University">
  
    <link rel="alternative" href="/atom.xml" title="SunShining" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  
</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">SunShining</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-home-icon" class="nav-icon" href="/"></a>
        
          <a id="nav-archives-icon" class="nav-icon" href="/archives"></a>
        
          <a id="nav-about-icon" class="nav-icon" href="/about"></a>
        
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
      </nav>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Some-Machine-Learning-Blogs" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    
      <header class="article-header">
        
  
    <a class="article-title" href="/2015/04/19/Some-Machine-Learning-Blogs/">Some Machine Learning Blogs</a>
  

      </header>
    
    <time class="article-date" datetime="2015-04-19T13:38:31.000Z" itemprop="datePublished">04-19-2015</time>
    
  </div>
  <div class="article-inner">
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="http://quantombone.blogspot.com/" target="_blank" rel="external">Blog by Tomasz Malisiewicz</a>(Phd@CMU Now PostDoc@MIT)<br><a href="https://saravananthirumuruganathan.wordpress.com/category/data-mining/" target="_blank" rel="external">Blog by  Saravanan Thirumuruganathan</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/04/19/Some-Machine-Learning-Blogs/" data-id="ci8oifzyh00093kts5dyfl3ba" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/blog/">blog</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Math-Resourses" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    
      <header class="article-header">
        
  
    <a class="article-title" href="/2015/04/12/Math-Resourses/">Math Resourses</a>
  

      </header>
    
    <time class="article-date" datetime="2015-04-12T12:46:57.000Z" itemprop="datePublished">04-12-2015</time>
    
  </div>
  <div class="article-inner">
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Linear_Algebra">Linear Algebra</h1><p><a href="http://cs229.stanford.edu/section/cs229-linalg.pdf" target="_blank" rel="external">Linear Algebra Review and Reference</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/04/12/Math-Resourses/" data-id="ci8oifzyr000l3ktsy8cjqi2e" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-An-Introduction-to-EM-Algorithm" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    
      <header class="article-header">
        
  
    <a class="article-title" href="/2015/04/09/An-Introduction-to-EM-Algorithm/">An Introduction to EM Algorithm</a>
  

      </header>
    
    <time class="article-date" datetime="2015-04-09T14:52:54.000Z" itemprop="datePublished">04-09-2015</time>
    
  </div>
  <div class="article-inner">
    <div class="article-entry" itemprop="articleBody">
      
        <p>在图模型中，如果我们所有的变量全都是observed的话，就可以用maxmum likelihood estimation来求解参数，但是现实世界是纷繁复杂的，很多时候我们并不能观测到所有的变量。就像上帝已一个无形的手把某一些变量的分布给隐藏了。而在上个世纪的统计牛人就提出了类似于EM算法来对隐变量进行估计和求解。</p>
<h1 id="K-Means_Algorithm">K-Means Algorithm</h1><p>比如，在一个分类问题中，通过有lable的训练数据，我们可以学习到一个决策边界，如果我们把训练数据的lable去掉，这时候我们还能够对未来的数据进行预测他们所属的lable吗？这就是聚类问题，一般我们可以使用K-Means算法，K-Means算法的idea是我们对训练数据所属的lable进行迭代，最初我们随机初始化一个中心，然后在每一次迭代中我们对训练数据Assign一个距离它最近的那个中心的lable作为它的lable。因此K-Means算法就如下所示：</p>
<blockquote>
<ul>
<li>Initialize <em>*clusering centroids</em>$\mu_1,\mu_2,…,\mu_k \in R^{n}$</li>
<li>Repeat until convergence{<br>  For every $i$, set $$c^{i} := argmin_{j} ||x^{\left(i\right)}-\mu<em>j||^{2}$$<br>  For eachj,set $$\mu_j := \frac{\sum\</em>{i=1}^{m}1{c^{\left(i\right)} = j}x^{\left(i\right)}}{\sum_{i=1}^{m}1{c^{\left(i\right)} = j}}}$$<br>}<br>在迭代收敛后，K-Means收敛到局部最小值。这里K-Means类似于Hard EM，因为其在每次迭代时直接为每个data赋一个lable。而不是已概率的形式进行赋值。下面我们就介绍EM算法在聚类中的一个例子，也就是<strong>高斯混合分布</strong>(Mixture of Gaussian)</li>
</ul>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/04/09/An-Introduction-to-EM-Algorithm/" data-id="ci8oifzyv000n3ktsgazvgcat" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/EM-algorithm/">EM algorithm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Unsupervised-learning/">Unsupervised learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Graphical-Models-Notes" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    
      <header class="article-header">
        
  
    <a class="article-title" href="/2015/04/07/Graphical-Models-Notes/">Graphical-Models-Notes</a>
  

      </header>
    
    <time class="article-date" datetime="2015-04-07T04:15:38.000Z" itemprop="datePublished">04-07-2015</time>
    
  </div>
  <div class="article-inner">
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Grapical_Model">Grapical Model</h2><h3 id="Probalistic_Graphical_Model_Representation">Probalistic Graphical Model Representation</h3><p><strong>NO FREE LUNCH theory</strong>: You must make assumptions before you learn anything!! If you do not make assumptions, you only can make predictions that appears in training data.</p>
<p><strong>Learning:</strong> Searching in hypothsis space == optimization a function.<br><strong>Machine learning good example</strong> 曲线拟合，我们的assumption是曲线时smooth的，如果给一堆数据：</p>
<table>
<thead>
<tr>
<th>Data</th>
<th>Lable</th>
</tr>
</thead>
<tbody>
<tr>
<td>000</td>
<td>1</td>
</tr>
<tr>
<td>001</td>
<td>0</td>
</tr>
<tr>
<td>010</td>
<td>0</td>
</tr>
<tr>
<td>011</td>
<td>0</td>
</tr>
<tr>
<td>100</td>
<td>1</td>
</tr>
<tr>
<td>111</td>
<td>?</td>
</tr>
</tbody>
</table>
<p>预测111的lable，这里如果我们不做什么假设的话是不可以得到lable的。</p>
<p><strong>Markov Random Field:</strong><br>$$P\left(X\right) = \frac{1}{Z}\prod_{cliques~c} \psi_c\left(x_c \right)$$</p>
<p>$$Z = \sum_x \prod_{cliques~c} \psi_c\left(x_c \right)$$</p>
<p>计算Z要花费指数的时间<br>一般情况下我们用$\log$来表示$\psi \left(x \right)$<br>$$\psi_c \left(x_c \right) = exp \left(-H_c\left(x_c \right) \right)$$<br>这里$H_c\left(x_c \right)$是能量函数。</p>
<p>于是$P\left(x\right) = \frac{1}{Z} exp \left(-\sum_{cliques~c} H_c\left(x_c\right) \right) = \frac{1}{Z}exp\left(-H\left(X\right)\right)$</p>
<p>$H\left(X \right)$ is called “free energy” function</p>
<p><strong>Ising Models</strong></p>
<p>Nodes are arranged in a regular topology (often a regular packing grid) and connected only to their geometric neighbours.<br>Energy function:<br>$$H\left(x\right) = \sum_{ij} \beta_{ij} x_i x_j + \sum_i \alpha_i x_i$$</p>
<p><strong>Relations beteen directed graph &amp; undirected graph</strong><br>两者不能相互转化。</p>
<p><strong>Exponential Family</strong><br>很多分布都可以看成指数家族的一个特例（Uniform distribution是一个例外），定义如下：<br>$$p\left(x|\eta\right) = h\left(x\right)exp \left(\eta^{T}T\left(x\right) - A\left(\eta \right)\right)$$<br>$$p\left(x|\eta\right) = \frac{1}{Z\left(\eta \right)}h\left(x\right)exp\left(\eta^{T} T\left(x\right) \right)$$</p>
<ul>
<li>Natural parameter $\eta$</li>
<li>Function $T\left(x\right)$ is the sufficient statistic(Given training data, 如果知道了其sufficient statistic，这样我们就可以把原来的training数据扔掉了)</li>
<li>Function $A\left(x\right) = \log Z\left(\eta \right)$is the log normalizer</li>
<li>Examples: Bernoulli, binomial/geometric/negative-binomial, Poisson, gamma, multinomial, Gaussian, …</li>
</ul>
<p><strong>Moments</strong><br>We can easily compute moments of any exponential family distribution by taking the derivatives of the log normalizer $A\left(\eta \right)$.<br>The $q^{th}$ derivative gives the $q^{th}$ centred moment.<br>$$\frac{dA\left(\eta \right)}{d \eta} = mean$$<br>$$\frac{d^{2}A\left(\eta \right)}{d^{2} \eta} = variance$$<br>当sufficient statistic是vector时，需要求偏导，而不是导数了。</p>
<p><strong>GLM</strong><br>Generalized Linear Models: p(y|x) is exponential family with conditional mean $ \mu_i= f_\i(\theta^{T}x)$.</p>
<h3 id="Parameter_Learning_in_Graphical_Models">Parameter Learning in Graphical Models</h3><p><strong>NO Latent variable</strong><br>Given training data, 我们对其进行建模，<strong>SUPPOESE</strong>这组数据服从某种模型（Gaussian,Multinomial…）<br>比如线性回归，我们可以认为$p\left(y|x\right) = gaussian\left(y|\theta^{T}x,\sigma^{2} \right)$<br>利用maxmum log likehood对参数进行求解。得到的参数$\theta$与loss function得到的一致。<br><strong>关于sufficient statistics的几点说明</strong></p>
<ul>
<li>In the examples above, the sufficient statistics were merely sums (counts) of the data:</li>
</ul>
<table>
<thead>
<tr>
<th>Distribution</th>
<th>Sufficient Statistics</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bernoulli:</td>
<td>number of heads, tails</td>
</tr>
<tr>
<td>Multinomial:</td>
<td>number of each type</td>
</tr>
<tr>
<td>Gaussian:</td>
<td>mean, mean-square</td>
</tr>
<tr>
<td>Regression:</td>
<td>correlations</td>
</tr>
</tbody>
</table>
<ul>
<li>this is true for all exponential family models: sufficient statistics are the average natural parameters.</li>
<li>只要指数家族模型才有简单的sufficient statistics<br><strong>Generative v.s Descriminative model</strong></li>
<li>Generative Model: 直接对$p\left(x,y\right)$进行建模，在预测时用bayes rule来推断条件概率$p\left(y|x\right)$。通过G-M可以做密度估计(density estimation)</li>
<li>Discriminative Model:直接对$p\left(y|x\right)$进行建模。目标驱动化，如做regression。</li>
</ul>
<p><strong>Generative model for classification</strong></p>
<ul>
<li>$p\left(x,y\right) = p\left(y\right)p\left(x|y\right)$这里$p\left(y\right)$时某类的prior information。$p\left(x|y\right)$是某一类的条件特征分布。</li>
<li>prior: Bernoulli or Multimomial。</li>
<li>class-conditional distribution:如果数据是continuous，一般使用Gaussian class-conditional.$p\left(x|y=k,\theta\right) = \frac{1}{|2\pi \sum^{\frac{1}{2}}|}exp(-\frac{1}{2} \left(\left(x-\mu_k\right) \sum^{-1} \left(x-\mu_k \right)\right))$（Gaussion discriminative model，在Andrew Ng的stanford课上有详细推导）</li>
<li>Using MLE，The maximum likelihood fit of a Gaussian to some data is the Gaussian whose mean is equal to the data mean and whose covariance is equal to the sample covariance.</li>
<li>如果data比较少，或者＃ of data &lt;&lt; dim of feature ，有一些以下Regularizations：</li>
</ul>
<ol>
<li>Assuming all covariance are the same!==&gt; fish discriminant analysis</li>
<li>Assuming the covariance matrix is diagnal…(boundary是linear的)</li>
<li>add a bit of the identity matrix to each sample covariance. This “fattens it up” in directions where there wasn’t enough data. Equivalent to using a “Wishart prior” on the covariance matrix.(这一块不是很理解)</li>
</ol>
<ul>
<li>既然已经有了$p\left(x|y,\theta\right)$，我们的目标是$p\left(y|x,\theta\right)$，通过bayes rule：<br>$$p\left(y=k|x,\theta\right) = \frac{e^{\beta_k^{T}x}}{\sum_je^{\beta_j^{T}x}}$$<br>此时$\beta_k = \mu_k^{T}\sum^{-1}x-\mu_k^{T}\sum_{-1}\mu_k /2+\log \pi_k$</li>
</ul>
<p><strong>Structure learning in bayesian network of tree structure</strong><br>原则上我们要search所有的组合，然后选择likelihood最大的那一个。但是，这个问题可以转换到一个传统的数据结构的问题，我们可以使用最大生成树！<br>既然可以使用最大生成树，我们就要来推导一下这里的edge的weight。likelihood function为:<br>$$l\left(\theta;D \right) = \sum_{x \in V_all}N\left(x\right)\log p\left(x\right) = \sum_x N\left(x\right)\left(\log \left(x_r\right)+\sum_{i\neq r}\log p\left(x_i|\pi_i\right)\right)$$</p>
<h3 id="Latent_Variable_Learning_in_Graphical_Models(Using_EM_algorithm)">Latent Variable Learning in Graphical Models(Using EM algorithm)</h3><h3 id="Inference_in_Graphical_Model">Inference in Graphical Model</h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/04/07/Graphical-Models-Notes/" data-id="ci8oifzyt000m3kts4dz4g7h0" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-acl-2014-tutorial-language-modeling-using-ml" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    
      <header class="article-header">
        
  
    <a class="article-title" href="/2015/04/06/acl-2014-tutorial-language-modeling-using-ml/">acl-2014-tutorial-language_modeling_using_ml</a>
  

      </header>
    
    <time class="article-date" datetime="2015-04-06T14:28:12.000Z" itemprop="datePublished">04-06-2015</time>
    
  </div>
  <div class="article-inner">
    <div class="article-entry" itemprop="articleBody">
      
        <p>要做AI的project，读了一些paper和tutorial，这里做一些记录。这篇是来自牛津大学NLP组关于language modeling的tutorial。名字叫<strong>New Directions in Vector Space Models of Meaning</strong>。tutorial两个目的：</p>
<ul>
<li>Learning vector representations for words</li>
<li>Learning how to compose them to get vector representations for phrases/sentences/documents<h3 id="Menu四部分:">Menu四部分:</h3></li>
</ul>
<ol>
<li>Distributional Semantics</li>
<li>Neural Distributed Representations</li>
<li>Semantic Composition</li>
<li>Last Words</li>
</ol>
<p>在这里我们对每个词用一个vector来表示。这样词与词之间的similarity关系就可以用一些距离度量函数来度量。比如<strong>Cosine Similarity</strong>:<br>$$\cos \left( x,y \right) = \frac{xy}{||x|| \times ||y||}$$<br>或者其他的相似度衡量方法:</p>
<ul>
<li>Euclidean Distance</li>
<li>Lin</li>
<li>Jaccard</li>
<li>Dice</li>
<li>KL divergence<br>有需要的时候再wiki一下吧。。<br>用Vector Representation看起来比较Intuitive motivated。这里分析一下好坏：</li>
<li>Pros: Easy to obtain, vectors are interpretable</li>
<li>Cons: Involves a large number of design choices (what weighting scheme? what similarity measure?)</li>
<li>Problems: Going from word to sentence representations is non-trivial, and no clear intuitions exist.<br>于是问题来了，我们怎么样避免这么多种选择来解决问题？Bengio在2003年提出了用神经网络的方法来对language进行表示。<blockquote>
<p>Twenty years ago log-linear models freed us from the shackles of simple multinomial parametrisations, but imposed the tyranny of feature engineering.</p>
</blockquote>
</li>
</ul>
<p>从基本的Log-linear model for classification开始考虑：<br>这里Feature: $\phi \left( x \right) \in R^{D}$，每一个weight$\lambda _k \in R^{D}$（$k \in {1,…K}$）。因此，我们可以如下建立模型：Given 一个instance $x$，判断其归属于哪一类的概率为：<br>$$p\left( C_k|x \right) = \frac{exp\left( \lambda _k^{T} \phi\left(x\right) \right)}{\sum_j^{k}exp\left( \lambda _j^{T} \phi \left( x\right) \right)}$$<br>Log-linear model的参数是$\lambda$，在training中我们需要计算Gradient：<br>$$\frac{\partial}{\partial \lambda_j}[-\log p\left(C_k|x \right)] = p\left(C_j|x \right)\phi \left(x \right) - \delta \left(j,k\right)\phi\left(x\right)$$<br>$\delta \left(j,k \right)$ is the Kronecker delta function which is 1 if $j = k$ and 0 otherwise。<br>有了log-linear model，我们用来提出log-linear language model：<br>Classify the next word $w_n$ given $w_n-1$, $w_n-2$</p>
<p>TBA…</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/04/06/acl-2014-tutorial-language-modeling-using-ml/" data-id="ci8oifzyb00023ktsgi6rtf5b" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/language-modeling/">language modeling</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/natural-language-proccessing/">natural language proccessing</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-PRML-reading-notes" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    
      <header class="article-header">
        
  
    <a class="article-title" href="/2015/04/05/PRML-reading-notes/">PRML reading notes</a>
  

      </header>
    
    <time class="article-date" datetime="2015-04-05T13:50:52.000Z" itemprop="datePublished">04-05-2015</time>
    
  </div>
  <div class="article-inner">
    <div class="article-entry" itemprop="articleBody">
      
        <p>PRML(Pattern Recognition and Machine Learning)这本书正式开始阅读是从去年九月份。从一开始看的晕晕乎乎，到现在能够看进去，这中间也走了不少弯路。现在重新读一遍，跟随着Toronto大学的<a href="http://www.cs.toronto.edu/~rsalakhu/STA4273_2015/" target="_blank" rel="external">统计机器学习</a>课程，以此blog记录一下阅读笔记。</p>
<h2 id="ch1_Introduction">ch1 Introduction</h2><p><img src="http://ww1.sinaimg.cn/mw690/6dfbc26fjw1eqv29oa539j20nb0ebgqv.jpg" alt="Machine learning: a big picture"><br>机器学习主要分为有监督的学习和无监督学习，它们之间的区别在于target是否给定。在有监督学习中，target给定，若是离散值，比如给一张动物的图片，判断它是小狗还是小猫，这类问题是分类问题（Classification）；若取连续值，比如预测上海某个小区的房价，则是回归问题（Regression）。而在无监督学习中，target不存在，这里有density estimation, clustering, dimensionality reduction, finding hidden explanation等等任务。<br><img src="http://ww2.sinaimg.cn/mw690/6dfbc26fjw1eqv2dl7cxej20y30frq72.jpg" alt="Classification: an example"></p>
<h3 id="ch_1-1_Exmaple:_Polynomial_Curve_Fitting">ch 1.1 Exmaple: Polynomial Curve Fitting</h3><p>我们从最简单的回归问题（曲线拟合）开始说起，Given 一系列训练数据$\left( x^{\left( 1\right) },x^{\left( 2\right) },…,x^{\left( N\right) }\right) ^{T}$, 每一个$x^{i}=x^{i}_1…x^{i}_d$。它们对应的target值$y=(y_1,…,y_N)^{T}$。我们认为$x$与$y$的关系是线性关系，因此，我们对每一个$x^{\left( i \right)}$的预测值<br>$${y(x^{\left( i \right)},w) = \left( x^{\left( i \right)} \right)^{T}w}$$<br>这里我们可以写成<br>$${y(X,w) = X^{T}w}$$<br>注意$X$是一个$N \times \left( d+1 \right)$的矩阵。<br>我们希望通过最小化一个loss function来获得最好的$w$，在这里定义loss function$E\left( w \right)$为squred error，<br>$$E\left( w \right) = \frac{1}{2} (Xw - y)^{T}(Xw - y)$$<br>如果$X^{T}X$是非奇异的矩阵，我们可以求出最优解<br>$$w^{*} = \left( X^{T}X \right)^{-1}X^{T}y$$<br>但是在实际中如果N很大，这里矩阵运算将非常耗时。在matlab下面做了个实验，当N=10000时，只是做$X^{T}X$就要耗时13秒。显然这种方法在大规模实际应用中不可取。<br>另外，这里的linear models是指关于coeffients $w$的线性函数，而不是关于$x$的线性函数。<br>好的，既然已经有了线性模型的解决方法，我们用它来对一个$\sin \left( 2\pi x\right)$进行拟合。我们从这个函数中sampling出来一些data，如下图<br><img src="http://ww2.sinaimg.cn/mw690/6dfbc26fjw1eqvt3fb3v2j20ak07v74e.jpg" alt="$\sin \left( 2\pi x\right)$"><br>我们用一个多项式的函数来拟合这些数据，</p>
<p>TBA…</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/04/05/PRML-reading-notes/" data-id="ci8oifzym000h3ktso0pc3km8" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PRML/">PRML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    
      <header class="article-header">
        
  
    <a class="article-title" href="/2015/04/05/hello-world/">Hello World</a>
  

      </header>
    
    <time class="article-date" datetime="2015-04-05T11:39:36.903Z" itemprop="datePublished">04-05-2015</time>
    
  </div>
  <div class="article-inner">
    <div class="article-entry" itemprop="articleBody">
      
        <p>感谢<a href="http://zhaoyanpeng.github.io/" target="_blank" rel="external">秦风师弟</a>的帮助，以后这里就是新家啦！</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/04/05/hello-world/" data-id="ci8oifzy600013ktsrbspzlrm" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Resources-of-machine-learning" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    
      <header class="article-header">
        
  
    <a class="article-title" href="/2015/04/05/Resources-of-machine-learning/">Resources of machine learning</a>
  

      </header>
    
    <time class="article-date" datetime="2015-04-05T11:10:15.000Z" itemprop="datePublished">04-05-2015</time>
    
  </div>
  <div class="article-inner">
    <div class="article-entry" itemprop="articleBody">
      
        <p>下面是搜集的一些机器学习的课程资源：</p>
<h2 id="1-_General_Machine_learning_courses">1. General Machine learning courses</h2><h3 id="1-1_Introduction_to_Machine_Learning_courses">1.1 Introduction to Machine Learning courses</h3><p>Coursera上 Andrew Ng的<a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="external">machine learning</a>课，现在是self learning，可以随时学习。</p>
<h3 id="1-2_Machine_Learning_courses_@_Universities_with_videos">1.2 Machine Learning courses @ Universities with videos</h3><p>Andrew Ng在Stanford开设的<a href="http://cs229.stanford.edu/" target="_blank" rel="external">CS 229: Machine Learning</a>课程，notes很好，多看几遍收获很大。<br>来自Washington的Pedro Domingos在Coursera上面开设的<a href="https://www.coursera.org/course/machlearning" target="_blank" rel="external">Machine Learning</a>，Domingos大牛可谓机器学习界风云人物，发明了Sum-Product Network, Markov Logic等等模型，我等渣渣只能在大牛的脚下做一些小的improvement。<br>CMU院士Tom Mitchell开设的<a href="http://www.cs.cmu.edu/~tom/10701_sp11/" target="_blank" rel="external">10-701/15-781</a></p>
<h3 id="1-3_Machine_Learning_courses_@_Universities_without_videos">1.3 Machine Learning courses @ Universities without videos</h3><p>MIT 6.867 研究生课程<a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/" target="_blank" rel="external">Machine Learning</a>，notes很详细！</p>
<h2 id="2-_Advanced_machine_learning_courses">2. Advanced machine learning courses</h2><p>高级机器学习课程，包括图模型，无参贝叶斯，统计机器学习理论</p>
<h3 id="2-1_Probabilistic_Graphical_Models">2.1 Probabilistic Graphical Models</h3><p>来自CMU的Eric Xing在2014年春季开设的概率图模型：<a href="http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html" target="_blank" rel="external">Probabilistic Graphical Models<br>(Spring 2014)</a></p>
<p>Stanford的Daphne Koller在coursera开设的<a href="https://www.coursera.org/course/pgm" target="_blank" rel="external">Probabilistic Graphical Models</a></p>
<h3 id="2-2_Statistical_Learning_Theory">2.2 Statistical Learning Theory</h3><p>CMU大牛Larry Wasserman的统计机器学习<a href="http://www.stat.cmu.edu/~larry/=sml/" target="_blank" rel="external">Statistical Machine Learning</a></p>
<h3 id="2-3_一些比较老的资源">2.3 一些比较老的资源</h3><h4 id="2-3-1_Copied_from_Dr-_Tomasz-">2.3.1 Copied from Dr. Tomasz.</h4><p>以下资源都是从<a href="http://www.cs.cmu.edu/~tmalisie/mllinks.html" target="_blank" rel="external">Dr. Tomasz</a>主页分享而获得的，<br><strong>With videos</strong><br><a href="http://helper.ipam.ucla.edu/schedule.aspx?pc=gss2005" target="_blank" rel="external">Graduate Summer School: Intelligent Extraction of Information from Graphs and High Dimensional Data.</a><br>UCLA Institute for Pure &amp; Applied Mathematics.<br>July 2005<br>(He highly recomment the Michael Jordan Graphical Model videos!!!)</p>
<p><a href="http://www.msri.org/workshops/298" target="_blank" rel="external">Emphasis Week on Learning and Inference in Vision</a><br>February 2005<br>Simoncelli, Mumford, Fitzgibbon, Efros, Frey, Zhu,<br>Freeman, Black, Blake, Isard, Weiss, Huttenlocher,<br>Yuille, Zabih, Besag, Gottardo, Donoho<br>MSRI</p>
<p><strong>With notes</strong><br><a href="http://www.cs.berkeley.edu/~jordan/courses/281B-spring04/" target="_blank" rel="external">CS 281B / Stat 241B: Statistical Learning Theory</a><br>Spring 2004<br>Michael Jordan<br>Berkeley</p>
<p><a href="http://www.cs.berkeley.edu/~jordan/courses/281A-fall04/" target="_blank" rel="external">CS 281A / Stat 241A: Statistical Learning Theory</a><br>Fall 2004<br>Michael Jordan<br>Berkeley</p>
<p><a href="http://www.mit.edu/~9.520/fall14/" target="_blank" rel="external">9.520: Statistical Learning Theory and Applications</a><br>Spring 2014<br>Tomas Poggio et al<br>MIT<br><a href="http://www.mit.edu/afs/athena/course/9/9.520/www/spring03/" target="_blank" rel="external">Spring 2003</a></p>
<p><a href="http://web.stanford.edu/class/stats315b/" target="_blank" rel="external">Statistics 315B: Modern Applied Statistics: </a><br>Elements of Statistical Learning II<br>Jerome H. Friedman<br>Stanford</p>
<p><a href="http://www.cs.ubc.ca/~murphyk/teaching/cs532c_fall04/index.html" target="_blank" rel="external">Probabilistic Graphical Models</a><br>Fall 2004<br>Kevin Murphy<br>UBC</p>
<h4 id="2-3-2_Some_Graduate_school_videos_and_old_workshops-">2.3.2 Some Graduate school videos and old workshops.</h4><p><strong>With videos</strong><br><a href="http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/DeepLearningWorkshopNIPS2007" target="_blank" rel="external">Deep Learning Workshop: Foundations and Future Directions</a><br><a href="http://courses.cs.washington.edu/courses/cse577/04sp/contents.html#BP" target="_blank" rel="external">http://courses.cs.washington.edu/courses/cse577/04sp/contents.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/04/05/Resources-of-machine-learning/" data-id="ci8oifzyj000d3ktsmw7mv4ha" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/">Machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/learning/">learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
  
</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 Yong Jiang<br>
      <a href="https://github.com/steven5538/hexo-theme-athena" target="_blank">Athena</a> by <a href="http://steven5538.tw" target="_blank">Steven5538</a> | Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} 
}); 
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
  </div>
</body>
</html>